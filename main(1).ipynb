{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2140,
     "status": "ok",
     "timestamp": 1628600812977,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "yCZMe8EJvZVm"
   },
   "outputs": [],
   "source": [
    "# ici, il importe les libraries commun de travail\n",
    "\n",
    "import datetime\n",
    "from csv import reader\n",
    "from datetime import datetime\n",
    "from math import exp\n",
    "from random import random\n",
    "from random import randrange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ici il prépare des fonctions standards qui vont ètre utisés fréquemment le long du projet\n",
    "\n",
    "\n",
    "\n",
    "# ici une fonction qui catégorise un attribut dont les valeurs sont pris dans la liste l \n",
    "def from_values_to_categorical(l):\n",
    "    \"\"\"\n",
    "  Convert a list of values to a list of categorical value\n",
    "  l : list to consider(list)\n",
    "  return : a list of categorical value (list of int)\n",
    "  \"\"\"\n",
    "    dictionnary = from_list_to_dic(l) # fonction qui donne un dictionnaire: la key: l'élement l et la valeur : c'est son index dans la liste\n",
    "    categorical = [dictionnary[key] for key in l] # la liste des indices des élements de l qui vont présenter les catégories\n",
    "    return categorical, dictionnary\n",
    "\n",
    "\n",
    "# ici la fonction qui donne un dictionnaire à partir d'une liste:\n",
    "# la key: c'est l'élement de l \n",
    "# la valeur : c'est son index dans la liste\n",
    "def from_list_to_dic(l):\n",
    "    \"\"\"\n",
    "  Creating a dictionnary  from a list of values\n",
    "  l : list of value\n",
    "  return : a dictionnaire associating a unique number to each value\n",
    "  \"\"\"\n",
    "    l = list(set(l))\n",
    "    n = len(l)\n",
    "    dictionnary = {}\n",
    "    for i in range(n):\n",
    "        dictionnary[l[i]] = i # voilà, ici il associe à chaque key (l[i]) sa valeur qui est i (son index)\n",
    "    return dictionnary\n",
    "\n",
    "\n",
    "# ici cette fonction convertit des dates en des nombres car les machines learning modèles ne traite que \n",
    "# des données numériques :\n",
    "# comment est cette convertion? pour chaque date, on calcule la période qui la sépare de 01-01-2000\n",
    "# Aprés, pour chaque période calculée, on calcule sa période qui la sépare du minimum de ces périodes.\n",
    "# donc, on obtient d'une liste des dates , des listes des nombres \n",
    "def from_date_to_number(l, dateFormatter):\n",
    "    \"\"\"\n",
    "  Convert a list of date to a list of float\n",
    "  l : list to consider (list of str)\n",
    "  dateFormatter : Format of the date (str)\n",
    "  return : a list of float (list of float)\n",
    "  \"\"\"\n",
    "    # l: liste contenant des chaines de caractères représentant la date\n",
    "    list_datev1 = []\n",
    "    for date in l:\n",
    "        date2 = datetime.strptime(date, dateFormatter) \n",
    "        #strptime: fonction de la librarie datetime qui prend la date sous forme string ainsi que le format de \n",
    "        # la date voulu et le converti en datetime object \n",
    "        date1 = (date2 - datetime(2000, 1, 1)).total_seconds()\n",
    "        #datetime.total_seconds(): calcule une différence des dates en secondes \n",
    "        # donc ici il calcule la période entre les dates à traiter et la date(01-01-2000) en secondes \n",
    "        list_datev1.append(date1)\n",
    "\n",
    "    min_date = min(list_datev1) # ici on prends la période minimale\n",
    "\n",
    "    list_date = [(date - min_date) for date in list_datev1]# liste des différences entre les différents périodes  \n",
    "    # calculées précedemment et la periode minimale\n",
    "    return list_date\n",
    "\n",
    "\n",
    "# Convertion rate for the currency \n",
    "# a 7*7 matrix containing currency rate \n",
    "currency_rate = np.array([[1, 0.85, 1.18, 1.08, 10.20, 10.49, 1.49], [1.17, 1, 1.39, 1.27, 11.97, 12.33, 1.74],\n",
    "                          [0.85, 0.72, 1, 0.91, 8.63, 8.88, 1.26], [0.93, 0.79, 1.09, 1, 9.44, 9.71, 1.37],\n",
    "                          [0.098, 0.083, 0.12, 0.11, 1, 1.03, 0.15], [0.095, 0.081, 0.11, 0.1, 0.97, 1, 0.14],\n",
    "                          [0.67, 0.57, 0.8, 0.73, 6.86, 7.07, 1]])\n",
    "\n",
    "\n",
    "\n",
    "# ici , on convertit la currency de la valeur des tickets en utilsant la matrice précedente\n",
    "def conversion_highticket(data):\n",
    "    \"\"\"\n",
    "  Apply the conversion rate to the dataframe\n",
    "  \"\"\"\n",
    "    if isinstance(data[6], (int, float)): # boolean function : check if data[6] is an integer or a float\n",
    "        currency1 = min(int(data[6]), 7)\n",
    "        currency2 = 1\n",
    "        amount = data[3] # 3rd column contient les prix highticket\n",
    "    return (amount * currency_rate[currency1 - 1][currency2 - 1])\n",
    "\n",
    "\n",
    "def conversion_lowticket(data):\n",
    "    \"\"\"\n",
    "  Apply the conversion rate thedataframe\n",
    "  \"\"\"\n",
    "    if isinstance(data[6], (int, float)): # boolean function : check if data[6] is an integer or a float\n",
    "        currency1 = min(int(data[6]), 7)\n",
    "        currency2 = 1\n",
    "        amount = data[2]# 2nd column contient les prix lowticket\n",
    "    return (amount * currency_rate[currency1 - 1][currency2 - 1])\n",
    "\n",
    "\n",
    "def conversion_lowinvest(data):\n",
    "    \"\"\"\n",
    "  Apply the conversion rate thedataframe\n",
    "  \"\"\"\n",
    "    if isinstance(data[6], (int, float)):\n",
    "        currency1 = min(int(data[6]), 7)\n",
    "        currency2 = 1\n",
    "        amount = data[4]# 4rd column contient les prix lowinvest\n",
    "    return (amount * currency_rate[currency1 - 1][currency2 - 1])\n",
    "\n",
    "\n",
    "def conversion_highinvest(data):\n",
    "    \"\"\"\n",
    "  Apply the conversion rate thedataframe\n",
    "  \"\"\"\n",
    "    if isinstance(data[6], (int, float)):\n",
    "        currency1 = min(int(data[6]), 7)\n",
    "        currency2 = 1\n",
    "        amount = data[5]# 3rd column contient les prix highinvest\n",
    "    return amount * currency_rate[currency1 - 1][currency2 - 1]\n",
    "\n",
    "# élimine les colonnes et les lignes en posant un certain threshold à partir duquel , il élimine \n",
    "# partie coding ici est claire\n",
    "def take_off_missing_value(data, column, percent, row):\n",
    "    \"\"\"\n",
    "  Take off the missing value from a panda dataframe\n",
    "  data : panda dataframe\n",
    "  columns : if we want to take of the columns with a certain percentage of value missing (boolean)\n",
    "  percent : percentage of value from which we take of the column(float between 0 and 100)\n",
    "  row : if we want to take of the rows with a certain percentage of value missing (boolean)\n",
    "  \"\"\"\n",
    "    column_with_nan = data.columns[data.isnull().any()]\n",
    "\n",
    "    # Taking off the column with more 30% of missing\n",
    "    if column:\n",
    "        for column1 in column_with_nan:\n",
    "            if data[column1].isnull().sum() * 100.0 / data.shape[0] > percent:\n",
    "                data.drop(column1, 1, inplace=True)\n",
    "\n",
    "    # Taking off the row with missing values\n",
    "    if row:\n",
    "        index_with_nan = data.index[data.isnull().any(axis=1)]\n",
    "        data.drop(index_with_nan, 0, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# il y a des valeurs manquantes dans la colonnes 'exist_date'\n",
    "# on les remplace par 31-12-9999 22:00:00\n",
    "def replacemissingexitsdate(x):\n",
    "    '''\n",
    "    Replace a missing exit date by ' 9999-12-31 22:00:00 '\n",
    "    :param x: a date (str)\n",
    "    :return: x if it's not empty, ' 9999-12-31 22:00:00 ' otherwise\n",
    "    '''\n",
    "    if x[1] == ' ':\n",
    "        return ' 9999-12-31 22:00:00 '\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "# pareil pour la colonne'entry_date'\n",
    "def replacemissingentrysdate(x):\n",
    "    '''\n",
    "    Replace a missing entry date by ' 0001-01-01 01:01:01 '\n",
    "    :param x: a date (str)\n",
    "    :return: x if it's not empty, ' 0001-01-01 01:01:01 ' otherwise\n",
    "    '''\n",
    "    if x[1] == ' ':\n",
    "        return ' 0001-01-01 01:01:01 '\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# replace all the missing values in id ('Nan value') by a 0\n",
    "def replace_missing_id(x):\n",
    "    '''\n",
    "    Replace a missing\n",
    "    :param x: the id\n",
    "    :return: x if it's not empty, 0 otherwise\n",
    "    '''\n",
    "    if type(x) == str: # pour dire qu'il s'agit d'un Nan value sous forme str\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "# Load a CSV file but put it in a list of list instead of a dataframe object\n",
    "# on sait pas pourquoi mais il a fait comme ça ici en fait.\n",
    "def load_csv(filename):\n",
    "    '''\n",
    "    Load a csv and put it in a list of list\n",
    "    :param filename: name of the csv file (str)\n",
    "    :return: a dataset (list of list)\n",
    "    '''\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Convert string column to float\n",
    "# pour les colonnes à valeurs str , on les change en des valeurs numériques\n",
    "def str_column_to_float(dataset, column):\n",
    "    '''\n",
    "\n",
    "    :param dataset: dataset to modify (list of list)\n",
    "    :param column: the name of the column (list of str)\n",
    "    :return: the same dataset with the name of the column converted into float\n",
    "    '''\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "\n",
    "# Convert string column to integer\n",
    "#pareil au précedent\n",
    "def str_column_to_int(dataset, column):\n",
    "    '''\n",
    "    Convert string column into integer\n",
    "    :param dataset: dataset to modify (list of list)\n",
    "    :param column: the name of the column (list of str)\n",
    "    :return: the same dataset with the name of the column converted into int\n",
    "    '''\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "\n",
    "# Find the min and max values for each column\n",
    "# chercher le smin et max de chaque colonnes et mettre chaque couple dans une liste et tous ces listes \n",
    "# dans une grande liste\n",
    "def dataset_minmax(dataset):\n",
    "    '''\n",
    "    Find the min and max values for each column\n",
    "    :param dataset: dataset to consider (list of list)\n",
    "    :return: minimum and maximum value in the dataset (list of float)\n",
    "    '''\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Rescale dataset columns to the range 0-1\n",
    "# il utilise la technique min_max scaling (connu en statistiques)\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    '''\n",
    "    Rescale dataset columns to the range 0-1\n",
    "    :param dataset: dataset to consider (list of list)\n",
    "    :param minmax: minimum and maximum value in the dataset (list of float)\n",
    "    :return: normalize dataset (list of list)\n",
    "    '''\n",
    "    for row in dataset:\n",
    "        for i in range(len(row) - 1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0]) # formule de min-max scaling\n",
    "\n",
    "\n",
    "# Split a dataset into k folds\n",
    "# pour utiliser les cross validation scoring method, on doit diviser le dataset en n folds\n",
    "# aprés utiliser chacune comme une training set et tous les autres come une validation set\n",
    "# continuer à faire ça en parcourant tous les n folds et calculer le score d'accuracy à chaque fois\n",
    "# puis donner le score moyen d'accuracy\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    '''\n",
    "\n",
    "    :param dataset: dataset to consider (list of list)\n",
    "    :param n_folds: number of folds to split the dataset in (int)\n",
    "    :return: dataset splitted into k folds (list of list of list)\n",
    "    '''\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "# coding littéral de l'acuuracy metric \n",
    "def accuracy_metric(actual, predicted):\n",
    "    '''\n",
    "    Calculate accuracy percentage\n",
    "    :param actual: the actual value (list of float)\n",
    "    :param predicted: the value predicted by the model (list of float)\n",
    "    :return: the accuracy percentage (float)\n",
    "    '''\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0 # ça donne l'accuracy score\n",
    "\n",
    "\n",
    "#sensitivity accuracy\n",
    "def sensitivity_metric(predicted, actual):\n",
    "    '''\n",
    "    Calculate sensitivity percentage\n",
    "    :param predicted: the value predicted by the model (list of float)\n",
    "    :param actual: the actual value (list of float)\n",
    "    :return: the sensitivity percentage (float)\n",
    "    '''\n",
    "    n_total = len(predicted)\n",
    "    n = 0\n",
    "    cpt = 0\n",
    "    for i in range(n_total):\n",
    "        if actual[i] == 1:\n",
    "            n += 1\n",
    "            if predicted[i] == 1:\n",
    "                cpt += 1\n",
    "    return cpt / float(len(actual)) * 100.0\n",
    "\n",
    "# voilà il utilise la technique de cross validation score : \n",
    "# Split a dataset into k folds\n",
    "# pour utiliser le cross validation scoring method, on doit diviser le dataset en n folds\n",
    "# aprés utiliser chacune comme une training set et tous les autres come une validation set\n",
    "# continuer à faire ça en parcourant tous les n folds et calculer le score d'accuracy à chaque fois\n",
    "# puis donner le score moyen d'accuracy\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    '''\n",
    "    Evaluate the performance of the algorithm over the different fold\n",
    "    :param dataset: dataset to consider (list of list)\n",
    "    :param algorithm: our machine learning model\n",
    "    :param n_folds: number of folds to split the data into (int)\n",
    "    :param args:\n",
    "    :return: the list of score of the model (list of float)\n",
    "    '''\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        sensitivity = sensitivity_metric(predicted, actual)\n",
    "        scores.append(sensitivity)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "# weighted somme (somme des inputs pondérés chacune par son poids(weight_vector w))\n",
    "def activate(weights, inputs):\n",
    "    '''\n",
    "    Calculate neuron activation for an input\n",
    "    :param weights: list of weights (list of float)\n",
    "    :param inputs: list of inputs (list of float)\n",
    "    :return: the value of the activation (float)\n",
    "    '''\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights) - 1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "\n",
    "# Transfer neuron activation\n",
    "# on utilise ici le sigmoid activation function\n",
    "def transfer(activation):\n",
    "    '''\n",
    "    Compute the activation function of the neuron\n",
    "    :param activation: the value of the neuron activation (float)\n",
    "    :return: value of the transfer function (float)\n",
    "    '''\n",
    "    return 1.0 / (1.0 + exp(-activation)) # sigmoid function\n",
    "\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "# on code littéralement le technique de forward propagation through neural network's layers\n",
    "def forward_propagate(network, row):\n",
    "    \"\"\"\n",
    "    Forward propagate input to a network output\n",
    "    :param network: our network\n",
    "    :param row: the input (list)\n",
    "    :return: our updated input\n",
    "    \"\"\"\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    \"\"\"\n",
    "    Calculate the derivative of an neuron output\n",
    "    :param output: a neuron output (float)\n",
    "    :return: derivative the neuron output float\n",
    "    \"\"\"\n",
    "    return output * (1.0 - output) \n",
    "\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "# ici on commence à coder le backpropagation technique (on enregistre les erreurs de forward propagation\n",
    "# dans les neurones en commençant du dernière couche jusqu'au première couche)\n",
    "def backward_propagate_error(network, expected):\n",
    "    \"\"\"\n",
    "\n",
    "    :param network: our neural network (list)\n",
    "    :param expected: the list of the expected value (list of float)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "# ça se répète ici : la mème fonction que prédemment\n",
    "def backward_propagate_error(network, expected):\n",
    "    \"\"\"\n",
    "    Backpropagate error and store in neurons\n",
    "    :param network: our neural network (list)\n",
    "    :param expected: a list of expected value (list of float)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network) - 1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "\n",
    "# Update network weights with error \n",
    "def update_weights(network, row, l_rate):\n",
    "    '''\n",
    "    Update network weights with error\n",
    "    :param network: our neural network (list of float)\n",
    "    :param row:\n",
    "    :param l_rate: the learning rate (float)\n",
    "    :return:\n",
    "    '''\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "# training the neural network function \n",
    "#étape 1: forward propagation \n",
    "#etape2: back propagation avec mise à jour des weights\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    \"\"\"\n",
    "    Train our neural network\n",
    "    :param network: our neural network (list of float)\n",
    "    :param train:\n",
    "    :param l_rate: our learning rate (float)\n",
    "    :param n_epoch: the number of epoch for the training (int)\n",
    "    :param n_outputs: the expected number of output (int)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    '''\n",
    "    Initialize a network\n",
    "    :param n_inputs: the number of inputs (int)\n",
    "    :param n_hidden: the number of hidden layer (int)\n",
    "    :param n_outputs: the number of output (int)\n",
    "    :return: a neural network with random weights (list of of dictionary)\n",
    "    '''\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    '''\n",
    "    Make a prediction with a network\n",
    "    :param network: our neural network (list of dictionary)\n",
    "    :param row:\n",
    "    :return: prediction of the neural network according the network\n",
    "    '''\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# function qui code littéralement la technique de back propagation\n",
    "def back_propagation(train, test, l_rate, n_epoch, n_hidden, list_network):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation in our neural network\n",
    "    :param train: our training dataset (list of list of float)\n",
    "    :param test: our test dataset (list of list of float)\n",
    "    :param l_rate: our learning rate (float)\n",
    "    :param n_epoch: the number of epoch of the training period (int)\n",
    "    :param n_hidden: the number of hidden layer (int)\n",
    "    :param list_network: a list of neural network (list of list of dict)\n",
    "    :return: the prediction made by the neural network (list of float)\n",
    "    \"\"\"\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    list_network.append(network)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "# scoring via le cross validation technique \n",
    "def find_best_mode(dataset, back_propagation, n_folds):\n",
    "    \"\"\"\n",
    "    Find the best feed forward neural for a given dataset.\n",
    "    Basically a fine tuning\n",
    "    :param dataset: our dataset (list of float)\n",
    "    :param back_propagation:\n",
    "    :param n_folds: the number of folds for the k fold validation\n",
    "    :return: the best model for our dataset(list of dict) and its sensitivity(float)\n",
    "    \"\"\"\n",
    "    list_rate = [0.001, 0.01, 0.1, 0.2, 0.5, 0.7, 1]\n",
    "    list_hidden = [1, 5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "    list_epoch = [1, 5, 10, 25, 100, 200, 500, 1000]\n",
    "    max_sensitivity = 0\n",
    "    for l_rate in list_rate:\n",
    "        for n_hidden in list_hidden:\n",
    "            for n_epoch in list_epoch:\n",
    "                list_network = []\n",
    "                scores = evaluate_algorithm(dataset, back_propagation, n_folds, l_rate, n_epoch, n_hidden, [])\n",
    "                if (sum(scores) / float(len(scores))) > max_sensitivity:\n",
    "                    max_sensitivity = (sum(scores) / float(len(scores)))\n",
    "                    if len(list_network) > 0:\n",
    "                        model = list_network[(len(list_network) - 1)]\n",
    "    return model, max_sensitivity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list de liste jusqu'à une dataset\n",
    "def from_list_to_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Convert a list of list into a usable dataset\n",
    "    :param dataset: a list of list of float with the last column being the output\n",
    "    :return: the input and the ouput (numpy.array)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in dataset:\n",
    "        l = []\n",
    "        n = len(line)\n",
    "    for i in range(n - 1):\n",
    "        l.append(line[i])\n",
    "    X.append(l)\n",
    "    Y.append(line[n - 1])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def padding(l):\n",
    "    m = 0\n",
    "    for element in l:\n",
    "        if len(element) > m:\n",
    "            m = len(element)\n",
    "    for element in l:\n",
    "        if len(element) < m:\n",
    "            rest = m - len(element)\n",
    "            for k in range(rest):\n",
    "                element.append(0)\n",
    "    return l\n",
    "\n",
    "# mean d'une liste\n",
    "def mean(l):\n",
    "    sum = 0\n",
    "    for element in l:\n",
    "        sum += element\n",
    "    return (sum/len(l))\n",
    "\n",
    "def min_list(l):\n",
    "    min = l[0]\n",
    "    for element in l:\n",
    "        if element < min:\n",
    "            min = element\n",
    "    return element\n",
    "\n",
    "def max_list(l):\n",
    "    max = l[0]\n",
    "    for element in l:\n",
    "        if element >max:\n",
    "            max = element\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1628600812982,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "DVq8ErelvePl"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "\n",
    "# ici cette fonction associe à un fund tous les stratégies qui lui correspond:\n",
    "def preprocessing_fund(strategy): \n",
    "    \"\"\"\n",
    "    Create ,from a dataframe containing all the strategies of the fund, a dictionary associating its fund to all of\n",
    "    its strategy :param strategy: name of the excel file (str) :return: a dictionary associating its fund to all of\n",
    "    its strategy\n",
    "    \"\"\"\n",
    "    strategy = pandas.read_excel(strategy)\n",
    "\n",
    "    strategy.columns = ['Fonds', 'investstrategyid', 'lowunitticket', 'highunitticket',\n",
    "                        'lowinvest', 'highinvest', 'currencyid', 'name', 'investstrategytypeid',\n",
    "                        'createddate', 'modifieddate', 'structureid', 'investstrategyid.1',\n",
    "                        'sectorid', 'Secteur', 'name.1', 'investstrategyid.2', 'localisationid',\n",
    "                        'Géo', 'investstrategyid.3', 'regionid', 'Region', 'investstrategyid.4',\n",
    "                        'operationtypeid', 'OperationType']\n",
    "\n",
    "    # Take off all the non numerical attribute\n",
    "    del strategy['createddate']\n",
    "    del strategy['modifieddate']\n",
    "    del strategy['Géo']\n",
    "    del strategy['Region']\n",
    "    del strategy['investstrategyid.3']\n",
    "    del strategy['investstrategyid.4']\n",
    "    del strategy['OperationType']\n",
    "    del strategy['investstrategyid.1']\n",
    "    del strategy['investstrategyid.2']\n",
    "    del strategy['Secteur']\n",
    "    del strategy['name.1']\n",
    "    del strategy['investstrategytypeid']\n",
    "    del strategy['name']\n",
    "    del strategy['structureid']\n",
    "\n",
    "    # Put everything under the same currency\n",
    "    strategy['lowunitticket'] = strategy.apply(conversion_lowticket, axis=1)\n",
    "    strategy['highunitticket'] = strategy.apply(conversion_highticket, axis=1)\n",
    "    strategy['lowinvest'] = strategy.apply(conversion_lowinvest, axis=1)\n",
    "    strategy['highinvest'] = strategy.apply(conversion_highinvest, axis=1)\n",
    "\n",
    "    del strategy['currencyid']\n",
    "    del strategy['operationtypeid']\n",
    "\n",
    "    # Replace the missing value\n",
    "    strategy['sectorid'] = strategy['sectorid'].transform(replace_missing_id)\n",
    "    strategy['localisationid'] = strategy['localisationid'].transform(replace_missing_id)\n",
    "    strategy['regionid'] = strategy['regionid'].transform(replace_missing_id)\n",
    "\n",
    "    # Since a strategy can contain many sector, localisation and region\n",
    "    # We're regrouping everything\n",
    "    list_fund = strategy.Fonds.values\n",
    "    list_strategyid = strategy.investstrategyid.values\n",
    "    list_lowunitticket = strategy.lowunitticket.values\n",
    "    list_highunitticket = strategy.highunitticket.values\n",
    "    list_highinvest = strategy.highinvest.values\n",
    "    list_lowinvest = strategy.lowinvest.values\n",
    "    list_sectorid = strategy.sectorid.values\n",
    "    list_localisationid = strategy.localisationid.values\n",
    "    list_regionid = strategy.regionid.values\n",
    "\n",
    "    fake_number_strat = len(list_strategyid)\n",
    "    list_index_strat = []\n",
    "    list_strat_done = []\n",
    "    for i in range(fake_number_strat):\n",
    "        strat = list_strategyid[i]\n",
    "        if strat not in list_strat_done:\n",
    "            l = []\n",
    "            list_strat_done.append(strat)\n",
    "            for j in range(fake_number_strat):\n",
    "                if (list_strategyid[j] == strat):\n",
    "                    l.append(j)\n",
    "            list_index_strat.append(l)\n",
    "\n",
    "    list_sectoridv1 = []\n",
    "    list_localisationidv1 = []\n",
    "    list_regionidv1 = []\n",
    "    list_highunitticketv1 = []\n",
    "    list_lowunitticketv1 = []\n",
    "    list_highinvestv1 = []\n",
    "    list_lowinvestv1 = []\n",
    "    list_fundv1 = []\n",
    "\n",
    "    for index in list_index_strat:\n",
    "        sector = []\n",
    "        localisation = []\n",
    "        region = []\n",
    "        highinvest = []\n",
    "        fund = []\n",
    "        for i in index:\n",
    "            sector.append(list_sectorid[i])\n",
    "            localisation.append(list_localisationid[i])\n",
    "            region.append(list_regionid[i])\n",
    "            if len(highinvest) == 0:\n",
    "                list_highunitticketv1.append(list_highunitticket[i])\n",
    "                list_lowunitticketv1.append(list_lowunitticket[i])\n",
    "                list_lowinvestv1.append(list_lowinvest[i])\n",
    "                list_highinvestv1.append(list_highinvest[i])\n",
    "                fund.append(list_fund[i])\n",
    "        list_sectoridv1.append(sector)\n",
    "        list_localisationidv1.append(localisation)\n",
    "        list_regionidv1.append(region)\n",
    "        list_fundv1.append(fund)\n",
    "\n",
    "    for i in range(len(list_sectoridv1)):\n",
    "        list_sectoridv1[i] = list(set(list_sectoridv1[i]))\n",
    "\n",
    "    for i in range(len(list_localisationidv1)):\n",
    "        list_localisationidv1[i] = list(set(list_localisationidv1[i]))\n",
    "\n",
    "    for i in range(len(list_regionidv1)):\n",
    "        list_regionidv1[i] = list(set(list_regionidv1[i]))\n",
    "\n",
    "    list_sectorid = padding(list_sectoridv1)\n",
    "    list_localisationid = padding(list_localisationidv1)\n",
    "    list_regionid = padding(list_regionidv1)\n",
    "    list_fund = []\n",
    "    for fund in list_fundv1:\n",
    "        for element in fund:\n",
    "            if element not in list_fund:\n",
    "                list_fund.append(element)\n",
    "\n",
    "    list_highunitticket = list_highunitticketv1\n",
    "    list_lowunitticket = list_lowunitticketv1\n",
    "    list_highinvest = list_highinvestv1\n",
    "    list_lowinvest = list_lowinvestv1\n",
    "\n",
    "    list_fundv1 = list(set(list_fund))\n",
    "    strategy_dictionnary = {}\n",
    "    for fund in list_fundv1:\n",
    "        strategy_dictionnary[fund] = []\n",
    "    for i in range(len(list_fund)):\n",
    "        fund = list_fund[i]\n",
    "        strategy_dictionnary[fund].append([list_sectorid[i], list_localisationid[i], list_regionid[i],\n",
    "                                      list_highunitticket[i],\n",
    "                                      list_lowunitticket[i], list_highinvest[i], list_lowinvest[i]])\n",
    "    return strategy_dictionnary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1628600812982,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "SW3gJVnQvmvo"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "import json\n",
    "\n",
    "# ici cette fonction crée un dictionnaire qui associe à chaque company ses valeurs clefs aprés les avoir transformé\n",
    "# en des données catégories.\n",
    "def preprocessing_company(excel, name_dictionary):\n",
    "\n",
    "    # Importing the file\n",
    "    companies = pandas.read_excel(excel)\n",
    "\n",
    "    # Taking off the missing values\n",
    "    companies = take_off_missing_value(companies, True, 30, True)\n",
    "\n",
    "    # Take off all the non numerical and useless attribute\n",
    "    del companies['createddate']\n",
    "    del companies['modifieddate']\n",
    "    del companies['street']\n",
    "    del companies['city']\n",
    "    del companies['zipcode']\n",
    "    del companies['secteur']\n",
    "    del companies['pays']\n",
    "    del companies['region']\n",
    "    del companies['secondarysectorid']\n",
    "    del companies['deactivateddate']\n",
    "    del companies['latitude']\n",
    "    del companies['longitude']\n",
    "    del companies['birthdate']\n",
    "    del companies['tcambegindate']\n",
    "    del companies['tcamenddate']\n",
    "\n",
    "    # We may use this one later\n",
    "    del companies['website']\n",
    "\n",
    "    # Renaming the columns\n",
    "\n",
    "    companies.columns = ['companyid', 'name', 'localisationid', 'sectorid', 'tcam', 'regionid',\n",
    "                         'isdeactivated', 'keyword', 'ca', 'ebitda']\n",
    "\n",
    "    # Convert the keyword into a categorical value\n",
    "    list_keyword = companies.keyword.values\n",
    "    list_name_company = companies.name.values\n",
    "    categorical_keyword = from_values_to_categorical(list_keyword)[0]\n",
    "\n",
    "    # Creating a dictionary of keyword for each company\n",
    "\n",
    "    dictionary_company_keyword = {}\n",
    "    for i in range(len(list_name_company)):\n",
    "        name_company = list_name_company[i]\n",
    "        dictionary_company_keyword[name_company] = []\n",
    "    max_numb_keyword = 0\n",
    "    for i in range(len(list_name_company)):\n",
    "        name_company = list_name_company[i]\n",
    "        dictionary_company_keyword[name_company].append(categorical_keyword[i])\n",
    "        if len(dictionary_company_keyword[name_company]) > max_numb_keyword:\n",
    "            max_numb_keyword = len(dictionary_company_keyword[name_company])\n",
    "\n",
    "    # All companies won't have the same number of keyword ,so we add zero\n",
    "    for company in dictionary_company_keyword.keys():\n",
    "        if len(dictionary_company_keyword[company]) < max_numb_keyword:\n",
    "            n_keys = max_numb_keyword - len(dictionary_company_keyword[company])\n",
    "            for j in range(n_keys):\n",
    "                dictionary_company_keyword[company].append(0)\n",
    "\n",
    "    # We take off the companies appearing twice\n",
    "    companies.drop_duplicates(subset=\"name\", keep='first', inplace=True)\n",
    "\n",
    "    def associate_key_to_value(key):\n",
    "        return dictionary_company_keyword[key]\n",
    "\n",
    "    companies['keyword'] = companies['name'].apply(associate_key_to_value)\n",
    "\n",
    "    # Converting non numerical attributes into categorical values\n",
    "    companies['isdeactivated'] = from_values_to_categorical(companies.isdeactivated.values)[0]\n",
    "    del companies['companyid']\n",
    "\n",
    "    # Converting the datafram into a dictionnary\n",
    "    company_dictionnary = companies.set_index('name').T.to_dict('list')\n",
    "\n",
    "    # Registering the dictionary\n",
    "    with open(name_dictionary, \"w\") as outfile:\n",
    "        json.dump(company_dictionnary, outfile)\n",
    "    return company_dictionnary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1628600812983,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "aIU8jIhxvnW2"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "import json\n",
    "\n",
    "# ici une fonction qui donne un dictionnaire associant à chaque company ses investors \n",
    "def preprocess_portfolio(excel, dictionary_company_portfolio_name, dictionary_fund_deal_name):\n",
    "    \"\"\"\n",
    "\n",
    "    :param excel: name of excel file of the portfolio (str)\n",
    "    :param dictionary_company_portfolio_name: name of the\n",
    "    dictionary containing each investors of a given company(str)\n",
    "    :param dictionary_fund_deal_name: name of the\n",
    "    dictionary containing each deal of a given fund (str)\n",
    "    :return: a dictionary containing each investors of a given\n",
    "    company and dictionary containing each deal of a given fund (dic)\n",
    "    \"\"\"\n",
    "    # Importing the file\n",
    "    portfolios = pandas.read_excel(excel)\n",
    "\n",
    "    # Take off all the non numerical and useless attributes\n",
    "\n",
    "    del portfolios['portfoliocompanyid']\n",
    "    del portfolios['companyid']\n",
    "    del portfolios['entryvalorisation']\n",
    "    del portfolios['modifieddate']\n",
    "    del portfolios['exitvalorisation']\n",
    "    del portfolios['structureid']\n",
    "    del portfolios['operationtype']\n",
    "    del portfolios['createddate']\n",
    "    del portfolios['builduprate']\n",
    "    del portfolios['isowned']\n",
    "    del portfolios['operationtypeid']\n",
    "    del portfolios['issearchaddon']\n",
    "\n",
    "    # Replacing the missing value\n",
    "    portfolios['exitdate'] = portfolios['exitdate'].transform(replacemissingexitsdate)\n",
    "    portfolios['entrydate'] = portfolios['entrydate'].transform(replacemissingentrysdate)\n",
    "\n",
    "    # Convert the date into numbers\n",
    "    portfolios['entrydate'] = from_date_to_number(portfolios.entrydate.values, ' %Y-%m-%d %H:%M:%S ')\n",
    "    portfolios['exitdate'] = from_date_to_number(portfolios.exitdate.values, ' %Y-%m-%d %H:%M:%S ')\n",
    "\n",
    "    # We're creating a dictionary containing each investors of a given company\n",
    "\n",
    "    company = portfolios.company.values\n",
    "    funds = portfolios.fonds.values\n",
    "    unique_company = list(set(company))\n",
    "    unique_funds = list(set(funds))\n",
    "\n",
    "    dictionary_company_portfolio = {}\n",
    "    for i in range(len(unique_company)):\n",
    "        name_company = unique_company[i]\n",
    "        dictionary_company_portfolio[name_company] = []\n",
    "\n",
    "    for i in range(len(company)):\n",
    "        name_company = company[i]\n",
    "        dictionary_company_portfolio[name_company].append(funds[i])\n",
    "\n",
    "    # We're creating a dictionary containing each deal of a given fund\n",
    "\n",
    "    list_entry_date = portfolios.entrydate.values\n",
    "    list_exit_date = portfolios.exitdate.values\n",
    "\n",
    "    dictionary_fund_deal = {}\n",
    "    for i in range(len(unique_funds)):\n",
    "        name_fund = unique_funds[i]\n",
    "        dictionary_fund_deal[name_fund] = []\n",
    "\n",
    "    for j in range(len(company)):\n",
    "        deal = [company[j], list_entry_date[j], list_exit_date[j]]\n",
    "        name_fund = funds[j]\n",
    "        dictionary_fund_deal[name_fund].append(deal)\n",
    "\n",
    "    # Registering the dictionaries\n",
    "    with open(dictionary_company_portfolio_name, \"w\") as outfile:\n",
    "        json.dump(dictionary_company_portfolio, outfile)\n",
    "\n",
    "    with open(dictionary_fund_deal_name, \"w\") as outfile:\n",
    "        json.dump(dictionary_fund_deal, outfile)\n",
    "    return dictionary_fund_deal, dictionary_company_portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1628600812986,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "xgfSVFanogfq"
   },
   "outputs": [],
   "source": [
    "def history(company, fund, dictionary_fund_deal, company_dictionary):\n",
    "    main_attributes = company_dictionary[company]\n",
    "    list_previous_company = [x[0] for x in dictionary_fund_deal[fund]]\n",
    "    list_last_deal = [x[2] for x in dictionary_fund_deal[fund]]\n",
    "    last_deal = max(list_last_deal)\n",
    "    nb_invest = len(dictionary_fund_deal[fund])\n",
    "    nb_local = 0\n",
    "    nb_region = 0\n",
    "    nb_sector = 0\n",
    "    list_ca = []\n",
    "    nb_comp_with_info = 0\n",
    "    for comp in list_previous_company:\n",
    "        if comp in company_dictionary.keys():\n",
    "            nb_comp_with_info += 1\n",
    "            attribut = company_dictionary[comp]\n",
    "            if attribut[0] == main_attributes[0]:\n",
    "                nb_local += 1\n",
    "            if attribut[1] == main_attributes[1]:\n",
    "                nb_sector += 1\n",
    "            if attribut[3] == main_attributes[3]:\n",
    "                nb_region += 1\n",
    "            list_ca.append(attribut[6])\n",
    "    if nb_comp_with_info > 0:\n",
    "        prop_local = nb_local / nb_invest\n",
    "        prop_sector = nb_sector / nb_invest\n",
    "        prop_region = nb_region / nb_invest\n",
    "        histo = [last_deal, nb_local, nb_sector, nb_region, max_list(list_ca),\n",
    "                 min_list(list_ca), mean(list_ca)]\n",
    "    else:\n",
    "        histo = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    return histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1628600812986,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "zUPcxjKTpWoS"
   },
   "outputs": [],
   "source": [
    "def matching(strategy_dictionnary, company_dictionnary, dictionary_company_portfolio, dictionary_fund_deal_name):\n",
    "    list_company = dictionary_company_portfolio.keys()\n",
    "    list_fund = dictionary_fund_deal_name.keys()\n",
    "    list_for_dataframe = []\n",
    "    print('Matching fund and company ')\n",
    "\n",
    "    # For each company\n",
    "    for company in list_company:\n",
    "\n",
    "        # For each fund\n",
    "        for fund in list_fund:\n",
    "\n",
    "            # If we have a deal involving this company\n",
    "            if company in company_dictionnary.keys():\n",
    "\n",
    "                # And a strategy related to this fund\n",
    "                if fund in strategy_dictionnary.keys():\n",
    "                    if fund in dictionary_company_portfolio[company]:\n",
    "                        isinvestor = True\n",
    "                    else :\n",
    "                        isinvestor = False\n",
    "                    list_strategy = strategy_dictionnary[fund]\n",
    "                    line = [company, fund]\n",
    "                    number_strategy = len(list_strategy)\n",
    "                    number_matching_strategy = 0\n",
    "                    histo = history(company, fund, dictionary_fund_deal_name, company_dictionnary)\n",
    "                    for element in histo:\n",
    "                        line.append(element)\n",
    "\n",
    "                    # For each strategy of the fund\n",
    "                    for strategy in list_strategy:\n",
    "                        matching = True\n",
    "                            # for attributes in strategy:\n",
    "                            # if (type(attributes) == float) or (type(attributes) == np.int64) or (\n",
    "                            # type(attributes) == int) or (type(attributes) == np.float64):\n",
    "                            # line.append(attributes)\n",
    "\n",
    "                            # else:\n",
    "                            # for aspect in attributes:\n",
    "                            # line.append(aspect)\n",
    "                            # for attributes in company_dictionnary[company]:\n",
    "                            # if (type(attributes)) == float or (type(attributes) == int):\n",
    "                            # line.append(attributes)\n",
    "                            # else:\n",
    "                            # for aspect in attributes:\n",
    "                            # line.append(aspect)\n",
    "\n",
    "                            # If the the sector from the company corresponds to one in the strategy\n",
    "                        if (company_dictionnary[company][1] not in strategy[0]) and (1 not in strategy[0]):\n",
    "                            matching = False\n",
    "\n",
    "                            # If the localisation\n",
    "                        if company_dictionnary[company][0] not in strategy[1]:\n",
    "                            matching = False\n",
    "\n",
    "                            # If the region\n",
    "                        if company_dictionnary[company][3] not in strategy[2]:\n",
    "                            matching = False\n",
    "                        if matching:\n",
    "                            number_matching_strategy += 1\n",
    "                    score_matching = number_matching_strategy / number_strategy\n",
    "                    line.append(number_matching_strategy)\n",
    "                    if isinvestor:\n",
    "                        line.append(1)\n",
    "                    else:\n",
    "                        line.append(0)\n",
    "                    del line[0]\n",
    "                    del line[0]\n",
    "                    list_for_dataframe.append(line)\n",
    "    return list_for_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 915121,
     "status": "ok",
     "timestamp": 1628601728094,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "fT5w5s3WpmhZ",
    "outputId": "14383099-134e-4587-91e3-a302f308b46c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching fund and company \n"
     ]
    }
   ],
   "source": [
    "# We first create a dictionary where we associate each fund to all of its strategy\n",
    "strategy_dictionary = preprocessing_fund('strategy.xlsx')\n",
    "\n",
    "# We create a dictionary where we associate each company to all of its attribute\n",
    "company_dictionary = preprocessing_company(\"companies.xlsx\", \"company_dictionnary.json\")\n",
    "\n",
    "# We now create 2 different dictionary\n",
    "# 1 associating each fund to all the companies he has already invested in\n",
    "# 1 associating each company to all the companies that have already invested in it\n",
    "\n",
    "dictionary_fund_deal_name, dictionary_company_portfolio = preprocess_portfolio(\"portfolioscompanies.xlsx\",\n",
    "                                                                               'dictionary_company_portfolio.json',\n",
    "                                                                               'dictionary_fund_deal_name.json')\n",
    "\n",
    "# We now create a list containing all the attribute of the strategy of the fund, all the attribute of the companies\n",
    "# and if the fund invested in the company or not (quite enormous)\n",
    "list_for_dataframe = matching(strategy_dictionary, company_dictionary, dictionary_company_portfolio,\n",
    "                              dictionary_fund_deal_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6751,
     "status": "ok",
     "timestamp": 1628601734835,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "7s0bBUk9zZVm"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from csv import reader\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1628601734839,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "Q3nSJxpuzdGy"
   },
   "outputs": [],
   "source": [
    "def from_list_to_dataset(dataset):\n",
    "  \"Convert a list into an usable dataset\"\n",
    "  X = []\n",
    "  Y = []\n",
    "  for line in dataset :\n",
    "    l = []\n",
    "    n = len(line)\n",
    "    for i in range(n-1):\n",
    "      l.append(float(line[i]))\n",
    "    X.append(np.asarray(l).astype(np.float32))\n",
    "    Y.append(float(line[n-1]))\n",
    "  return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1628601734840,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "T_KAcUMNzd6J"
   },
   "outputs": [],
   "source": [
    "def sensitivity(y_true, y_pred): \n",
    "  \"Compute the sensitivity of the classifier\"\n",
    "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "  return true_positives / (possible_positives + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1628602838171,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "cBmnsVP6zwxc"
   },
   "outputs": [],
   "source": [
    "dataset = list_for_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1628602429671,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "BMWzV3IWRNei",
    "outputId": "9c9db375-b9c8-4353-9d1b-18efe7fd00ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5617120,
     "status": "error",
     "timestamp": 1628608456535,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "b3EAE4v7z50z",
    "outputId": "7b10fed6-3184-44eb-ac17-95a51eb66d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
      "11100121\n",
      "11100186\n",
      "11100198\n",
      "11100202\n",
      "11100269\n",
      "11100358\n",
      "11100361\n",
      "11100365\n",
      "11100449\n",
      "11100475\n",
      "11100484\n",
      "11100502\n",
      "11100509\n",
      "11100547\n",
      "11100581\n",
      "11100614\n",
      "11100664\n",
      "11100700\n",
      "11100702\n",
      "11100731\n",
      "11100744\n",
      "11100748\n",
      "11100749\n",
      "11100770\n",
      "11100772\n",
      "11100812\n",
      "11100862\n",
      "11100885\n",
      "11100912\n",
      "11100914\n",
      "11100954\n",
      "11100957\n",
      "11100964\n",
      "11100972\n",
      "11100974\n",
      "11100978\n",
      "11100985\n",
      "11100991\n",
      "11101056\n",
      "11101068\n",
      "11101072\n",
      "11101139\n",
      "11101228\n",
      "11101231\n",
      "11101235\n",
      "11101319\n",
      "11101345\n",
      "11101354\n",
      "11101372\n",
      "11101379\n",
      "11101417\n",
      "11101451\n",
      "11101484\n",
      "11101534\n",
      "11101570\n",
      "11101572\n",
      "11101601\n",
      "11101614\n",
      "11101618\n",
      "11101619\n",
      "11101640\n",
      "11101642\n",
      "11101682\n",
      "11101732\n",
      "11101755\n",
      "11101782\n",
      "11101784\n",
      "11101824\n",
      "11101827\n",
      "11101834\n",
      "11101842\n",
      "11101844\n",
      "11101848\n",
      "11101855\n",
      "11101861\n",
      "11101926\n",
      "11101938\n",
      "11101942\n",
      "11102009\n",
      "11102098\n",
      "11102101\n",
      "11102105\n",
      "11102189\n",
      "11102215\n",
      "11102224\n",
      "11102242\n",
      "11102249\n",
      "11102287\n",
      "11102321\n",
      "11102354\n",
      "11102404\n",
      "11102440\n",
      "11102442\n",
      "11102471\n",
      "11102484\n",
      "11102488\n",
      "11102489\n",
      "11102510\n",
      "11102512\n",
      "11102552\n",
      "11102602\n",
      "11102625\n",
      "11102652\n",
      "11102654\n",
      "11102694\n",
      "11102697\n",
      "11102704\n",
      "11102712\n",
      "11102714\n",
      "11102718\n",
      "11102725\n",
      "11102731\n",
      "11102796\n",
      "11102808\n",
      "11102812\n",
      "11102879\n",
      "11102968\n",
      "11102971\n",
      "11102975\n",
      "11103059\n",
      "11103085\n",
      "11103094\n",
      "11103112\n",
      "11103119\n",
      "11103157\n",
      "11103191\n",
      "11103224\n",
      "11103274\n",
      "11103310\n",
      "11103312\n",
      "11103341\n",
      "11103354\n",
      "11103358\n",
      "11103359\n",
      "11103380\n",
      "11103382\n",
      "11103422\n",
      "11103472\n",
      "11103495\n",
      "11103522\n",
      "11103524\n",
      "11103564\n",
      "11103567\n",
      "11103574\n",
      "11103582\n",
      "11103584\n",
      "11103588\n",
      "11103595\n",
      "11103601\n",
      "11103666\n",
      "11103678\n",
      "11103682\n",
      "11103749\n",
      "11103838\n",
      "11103841\n",
      "11103845\n",
      "11103929\n",
      "11103955\n",
      "11103964\n",
      "11103982\n",
      "11103989\n",
      "11104027\n",
      "11104061\n",
      "11104094\n",
      "11104144\n",
      "11104180\n",
      "11104182\n",
      "11104211\n",
      "11104224\n",
      "11104228\n",
      "11104229\n",
      "11104250\n",
      "11104252\n",
      "11104292\n",
      "11104342\n",
      "11104365\n",
      "11104392\n",
      "11104394\n",
      "11104434\n",
      "11104437\n",
      "11104444\n",
      "11104452\n",
      "11104454\n",
      "11104458\n",
      "11104465\n",
      "11104471\n",
      "11104536\n",
      "11104548\n",
      "11104552\n",
      "11104619\n",
      "11104708\n",
      "11104711\n",
      "11104715\n",
      "11104799\n",
      "11104825\n",
      "11104834\n",
      "11104852\n",
      "11104859\n",
      "11104897\n",
      "11104931\n",
      "11104964\n",
      "11105014\n",
      "11105050\n",
      "11105052\n",
      "11105081\n",
      "11105094\n",
      "11105098\n",
      "11105099\n",
      "11105120\n",
      "11105122\n",
      "11105162\n",
      "11105212\n",
      "11105235\n",
      "11105262\n",
      "11105264\n",
      "11105304\n",
      "11105307\n",
      "11105314\n",
      "11105322\n",
      "11105324\n",
      "11105328\n",
      "11105335\n",
      "11105341\n",
      "11105406\n",
      "11105418\n",
      "11105422\n",
      "11105489\n",
      "11105578\n",
      "11105581\n",
      "11105585\n",
      "11105669\n",
      "11105695\n",
      "11105704\n",
      "11105722\n",
      "11105729\n",
      "11105767\n",
      "11105801\n",
      "11105834\n",
      "11105884\n",
      "11105920\n",
      "11105922\n",
      "11105951\n",
      "11105964\n",
      "11105968\n",
      "11105969\n",
      "11105990\n",
      "11105992\n",
      "11106032\n",
      "11106082\n",
      "11106105\n",
      "11106132\n",
      "11106134\n",
      "11106174\n",
      "11106177\n",
      "11106184\n",
      "11106192\n",
      "11106194\n",
      "11106198\n",
      "11106205\n",
      "11106211\n",
      "11106276\n",
      "11106288\n",
      "11106292\n",
      "11106359\n",
      "11106448\n",
      "11106451\n",
      "11106455\n",
      "11106539\n",
      "11106565\n",
      "11106574\n",
      "11106592\n",
      "11106599\n",
      "11106637\n",
      "11106671\n",
      "11106704\n",
      "11106754\n",
      "11106790\n",
      "11106792\n",
      "11106821\n",
      "11106834\n",
      "11106838\n",
      "11106839\n",
      "11106860\n",
      "11106862\n",
      "11106902\n",
      "11106952\n",
      "11106975\n",
      "11107002\n",
      "11107004\n",
      "11107044\n",
      "11107047\n",
      "11107054\n",
      "11107062\n",
      "11107064\n",
      "11107068\n",
      "11107075\n",
      "11107081\n",
      "11107146\n",
      "11107158\n",
      "11107162\n",
      "11107229\n",
      "11107318\n",
      "11107321\n",
      "11107325\n",
      "11107409\n",
      "11107435\n",
      "11107444\n",
      "11107462\n",
      "11107469\n",
      "11107507\n",
      "11107541\n",
      "11107574\n",
      "11107624\n",
      "11107660\n",
      "11107662\n",
      "11107691\n",
      "11107704\n",
      "11107708\n",
      "11107709\n",
      "11107730\n",
      "11107732\n",
      "11107772\n",
      "11107822\n",
      "11107845\n",
      "11107872\n",
      "11107874\n",
      "11107914\n",
      "11107917\n",
      "11107924\n",
      "11107932\n",
      "11107934\n",
      "11107938\n",
      "11107945\n",
      "11107951\n",
      "11108016\n",
      "11108028\n",
      "11108032\n",
      "11108099\n",
      "11108188\n",
      "11108191\n",
      "11108195\n",
      "11108279\n",
      "11108305\n",
      "11108314\n",
      "11108332\n",
      "11108339\n",
      "11108377\n",
      "11108411\n",
      "11108444\n",
      "11108494\n",
      "11108530\n",
      "11108532\n",
      "11108561\n",
      "11108574\n",
      "11108578\n",
      "11108579\n",
      "11108600\n",
      "11108602\n",
      "11108642\n",
      "11108692\n",
      "11108715\n",
      "11108742\n",
      "11108744\n",
      "11108784\n",
      "11108787\n",
      "11108794\n",
      "11108802\n",
      "11108804\n",
      "11108808\n",
      "11108815\n",
      "11108821\n",
      "11108886\n",
      "11108898\n",
      "11108902\n",
      "11108969\n",
      "11109058\n",
      "11109061\n",
      "11109065\n",
      "11109149\n",
      "11109175\n",
      "11109184\n",
      "11109202\n",
      "11109209\n",
      "11109247\n",
      "11109281\n",
      "11109314\n",
      "11109364\n",
      "11109400\n",
      "11109402\n",
      "11109431\n",
      "11109444\n",
      "11109448\n",
      "11109449\n",
      "11109470\n",
      "11109472\n",
      "11109512\n",
      "11109562\n",
      "11109585\n",
      "11109612\n",
      "11109614\n",
      "11109654\n",
      "11109657\n",
      "11109664\n",
      "11109672\n",
      "11109674\n",
      "11109678\n",
      "11109685\n",
      "11109691\n",
      "11109756\n",
      "11109768\n",
      "11109772\n",
      "11109839\n",
      "11109928\n",
      "11109931\n",
      "11109935\n",
      "11110019\n",
      "11110045\n",
      "11110054\n",
      "11110072\n",
      "11110079\n",
      "11110117\n",
      "11110151\n",
      "11110184\n",
      "11110234\n",
      "11110270\n",
      "11110272\n",
      "11110301\n",
      "11110314\n",
      "11110318\n",
      "11110319\n",
      "11110340\n",
      "11110342\n",
      "11110382\n",
      "11110432\n",
      "11110455\n",
      "11110482\n",
      "11110484\n",
      "11110524\n",
      "11110527\n",
      "11110534\n",
      "11110542\n",
      "11110544\n",
      "11110548\n",
      "11110555\n",
      "11110561\n",
      "11110626\n",
      "11110638\n",
      "11110642\n",
      "11110709\n",
      "11110798\n",
      "11110801\n",
      "11110805\n",
      "11110889\n",
      "11110915\n",
      "11110924\n",
      "11110942\n",
      "11110949\n",
      "11110987\n",
      "11111021\n",
      "11111054\n",
      "11111104\n",
      "11111140\n",
      "11111142\n",
      "11111171\n",
      "11111184\n",
      "11111188\n",
      "11111189\n",
      "11111210\n",
      "11111212\n",
      "11111252\n",
      "11111302\n",
      "11111325\n",
      "11111352\n",
      "11111354\n",
      "11111394\n",
      "11111397\n",
      "11111404\n",
      "11111412\n",
      "11111414\n",
      "11111418\n",
      "11111425\n",
      "11111431\n",
      "11111496\n",
      "11111508\n",
      "11111512\n",
      "11111579\n",
      "11111668\n",
      "11111671\n",
      "11111675\n",
      "11111759\n",
      "11111785\n",
      "11111794\n",
      "11111812\n",
      "11111819\n",
      "11111857\n",
      "11111891\n",
      "11111924\n",
      "11111974\n",
      "11112010\n",
      "11112012\n",
      "11112041\n",
      "11112054\n",
      "11112058\n",
      "11112059\n",
      "11112080\n",
      "11112082\n",
      "11112122\n",
      "11112172\n",
      "11112195\n",
      "11112222\n",
      "11112224\n",
      "11112264\n",
      "11112267\n",
      "11112274\n",
      "11112282\n",
      "11112284\n",
      "11112288\n",
      "11112295\n",
      "11112301\n",
      "11112366\n",
      "11112378\n",
      "11112382\n",
      "11112449\n",
      "11112538\n",
      "11112541\n",
      "11112545\n",
      "11112629\n",
      "11112655\n",
      "11112664\n",
      "11112682\n",
      "11112689\n",
      "11112727\n",
      "11112761\n",
      "11112794\n",
      "11112844\n",
      "11112880\n",
      "11112882\n",
      "11112911\n",
      "11112924\n",
      "11112928\n",
      "11112929\n",
      "11112950\n",
      "11112952\n",
      "11112992\n",
      "11113042\n",
      "11113065\n",
      "11113092\n",
      "11113094\n",
      "11113134\n",
      "11113137\n",
      "11113144\n",
      "11113152\n",
      "11113154\n",
      "11113158\n",
      "11113165\n",
      "11113171\n",
      "11113236\n",
      "11113248\n",
      "11113252\n",
      "11113319\n",
      "11113408\n",
      "11113411\n",
      "11113415\n",
      "11113499\n",
      "11113525\n",
      "11113534\n",
      "11113552\n",
      "11113559\n",
      "11113597\n",
      "11113631\n",
      "11113664\n",
      "11113714\n",
      "11113750\n",
      "11113752\n",
      "11113781\n",
      "11113794\n",
      "11113798\n",
      "11113799\n",
      "11113820\n",
      "11113822\n",
      "11113862\n",
      "11113912\n",
      "11113935\n",
      "11113962\n",
      "11113964\n",
      "11114004\n",
      "11114007\n",
      "11114014\n",
      "11114022\n",
      "11114024\n",
      "11114028\n",
      "11114035\n",
      "11114041\n",
      "11114106\n",
      "11114118\n",
      "11114122\n",
      "11114189\n",
      "11114278\n",
      "11114281\n",
      "11114285\n",
      "11114369\n",
      "11114395\n",
      "11114404\n",
      "11114422\n",
      "11114429\n",
      "11114467\n",
      "11114501\n",
      "11114534\n",
      "11114584\n",
      "11114620\n",
      "11114622\n",
      "11114651\n",
      "11114664\n",
      "11114668\n",
      "11114669\n",
      "11114690\n",
      "11114692\n",
      "11114732\n",
      "11114782\n",
      "11114805\n",
      "11114832\n",
      "11114834\n",
      "11114874\n",
      "11114877\n",
      "11114884\n",
      "11114892\n",
      "11114894\n",
      "11114898\n",
      "11114905\n",
      "11114911\n",
      "11114976\n",
      "11114988\n",
      "11114992\n",
      "11115059\n",
      "11115148\n",
      "11115151\n",
      "11115155\n",
      "11115239\n",
      "11115265\n",
      "11115274\n",
      "11115292\n",
      "11115299\n",
      "11115337\n",
      "11115371\n",
      "11115404\n",
      "11115454\n",
      "11115490\n",
      "11115492\n",
      "11115521\n",
      "11115534\n",
      "11115538\n",
      "11115539\n",
      "11115560\n",
      "11115562\n",
      "11115602\n",
      "11115652\n",
      "11115675\n",
      "11115702\n",
      "11115704\n",
      "11115744\n",
      "11115747\n",
      "11115754\n",
      "11115762\n",
      "11115764\n",
      "11115768\n",
      "11115775\n",
      "11115781\n",
      "11115846\n",
      "11115858\n",
      "11115862\n",
      "11115929\n",
      "11116018\n",
      "11116021\n",
      "11116025\n",
      "11116109\n",
      "11116135\n",
      "11116144\n",
      "11116162\n",
      "11116169\n",
      "11116207\n",
      "11116241\n",
      "11116274\n",
      "11116324\n",
      "11116360\n",
      "11116362\n",
      "11116391\n",
      "11116404\n",
      "11116408\n",
      "11116409\n",
      "11116430\n",
      "11116432\n",
      "11116472\n",
      "11116522\n",
      "11116545\n",
      "11116572\n",
      "11116574\n",
      "11116614\n",
      "11116617\n",
      "11116624\n",
      "11116632\n",
      "11116634\n",
      "11116638\n",
      "11116645\n",
      "11116651\n",
      "11116716\n",
      "11116728\n",
      "11116732\n",
      "11116799\n",
      "11116888\n",
      "11116891\n",
      "11116895\n",
      "11116979\n",
      "11117005\n",
      "11117014\n",
      "11117032\n",
      "11117039\n",
      "11117077\n",
      "11117111\n",
      "11117144\n",
      "11117194\n",
      "11117230\n",
      "11117232\n",
      "11117261\n",
      "11117274\n",
      "11117278\n",
      "11117279\n",
      "11117300\n",
      "11117302\n",
      "11117342\n",
      "11117392\n",
      "11117415\n",
      "11117442\n",
      "11117444\n",
      "11117484\n",
      "11117487\n",
      "11117494\n",
      "11117502\n",
      "11117504\n",
      "11117508\n",
      "11117515\n",
      "11117521\n",
      "11117586\n",
      "11117598\n",
      "11117602\n",
      "11117669\n",
      "11117758\n",
      "11117761\n",
      "11117765\n",
      "11117849\n",
      "11117875\n",
      "11117884\n",
      "11117902\n",
      "11117909\n",
      "11117947\n",
      "11117981\n",
      "11118014\n",
      "11118064\n",
      "11118100\n",
      "11118102\n",
      "11118131\n",
      "11118144\n",
      "11118148\n",
      "11118149\n",
      "11118170\n",
      "11118172\n",
      "11118212\n",
      "11118262\n",
      "11118285\n",
      "11118312\n",
      "11118314\n",
      "11118354\n",
      "11118357\n",
      "11118364\n",
      "11118372\n",
      "11118374\n",
      "11118378\n",
      "11118385\n",
      "11118391\n",
      "11118456\n",
      "11118468\n",
      "11118472\n",
      "11118539\n",
      "11118628\n",
      "11118631\n",
      "11118635\n",
      "11118719\n",
      "11118745\n",
      "11118754\n",
      "11118772\n",
      "11118779\n",
      "11118817\n",
      "11118851\n",
      "11118884\n",
      "11118934\n",
      "11118970\n",
      "11118972\n",
      "11119001\n",
      "11119014\n",
      "11119018\n",
      "11119019\n",
      "11119040\n",
      "11119042\n",
      "11119082\n",
      "11119132\n",
      "11119155\n",
      "11119182\n",
      "11119184\n",
      "11119224\n",
      "11119227\n",
      "11119234\n",
      "11119242\n",
      "11119244\n",
      "11119248\n",
      "11119255\n",
      "11119261\n",
      "11119326\n",
      "11119338\n",
      "11119342\n",
      "11119409\n",
      "11119498\n",
      "11119501\n",
      "11119505\n",
      "11119589\n",
      "11119615\n",
      "11119624\n",
      "11119642\n",
      "11119649\n",
      "11119687\n",
      "11119721\n",
      "11119754\n",
      "11119804\n",
      "11119840\n",
      "11119842\n",
      "11119871\n",
      "11119884\n",
      "11119888\n",
      "11119889\n",
      "11119910\n",
      "11119912\n",
      "11119952\n",
      "11120002\n",
      "11120025\n",
      "11120052\n",
      "11120054\n",
      "11120094\n",
      "11120097\n",
      "11120104\n",
      "11120112\n",
      "11120114\n",
      "11120118\n",
      "11120125\n",
      "11120131\n",
      "11120196\n",
      "11120208\n",
      "11120212\n",
      "11120279\n",
      "11120368\n",
      "11120371\n",
      "11120375\n",
      "11120459\n",
      "11120485\n",
      "11120494\n",
      "11120512\n",
      "11120519\n",
      "11120557\n",
      "11120591\n",
      "11120624\n",
      "11120674\n",
      "11120710\n",
      "11120712\n",
      "11120741\n",
      "11120754\n",
      "11120758\n",
      "11120759\n",
      "11120780\n",
      "11120782\n",
      "11120822\n",
      "11120872\n",
      "11120895\n",
      "11120922\n",
      "11120924\n",
      "11120964\n",
      "11120967\n",
      "11120974\n",
      "11120982\n",
      "11120984\n",
      "11120988\n",
      "11120995\n",
      "11121001\n",
      "11121066\n",
      "11121078\n",
      "11121082\n",
      "11121149\n",
      "11121238\n",
      "11121241\n",
      "11121245\n",
      "11121329\n",
      "11121355\n",
      "11121364\n",
      "11121382\n",
      "11121389\n",
      "11121427\n",
      "11121461\n",
      "11121494\n",
      "11121544\n",
      "11121580\n",
      "11121582\n",
      "11121611\n",
      "11121624\n",
      "11121628\n",
      "11121629\n",
      "11121650\n",
      "11121652\n",
      "11121692\n",
      "11121742\n",
      "11121765\n",
      "11121792\n",
      "11121794\n",
      "11121834\n",
      "11121837\n",
      "11121844\n",
      "11121852\n",
      "11121854\n",
      "11121858\n",
      "11121865\n",
      "11121871\n",
      "11121936\n",
      "11121948\n",
      "11121952\n",
      "11122019\n",
      "11122108\n",
      "11122111\n",
      "11122115\n",
      "11122199\n",
      "11122225\n",
      "11122234\n",
      "11122252\n",
      "11122259\n",
      "11122297\n",
      "11122331\n",
      "11122364\n",
      "11122414\n",
      "11122450\n",
      "11122452\n",
      "11122481\n",
      "11122494\n",
      "11122498\n",
      "11122499\n",
      "11122520\n",
      "11122522\n",
      "11122562\n",
      "11122612\n",
      "11122635\n",
      "11122662\n",
      "11122664\n",
      "11122704\n",
      "11122707\n",
      "11122714\n",
      "11122722\n",
      "11122724\n",
      "11122728\n",
      "11122735\n",
      "11122741\n",
      "11122806\n",
      "11122818\n",
      "11122822\n",
      "11122889\n",
      "11122978\n",
      "11122981\n",
      "11122985\n",
      "11123069\n",
      "11123095\n",
      "11123104\n",
      "11123122\n",
      "11123129\n",
      "11123167\n",
      "11123201\n",
      "11123234\n",
      "11123284\n",
      "11123320\n",
      "11123322\n",
      "11123351\n",
      "11123364\n",
      "11123368\n",
      "11123369\n",
      "11123390\n",
      "11123392\n",
      "11123432\n",
      "11123482\n",
      "11123505\n",
      "11123532\n",
      "11123534\n",
      "11123574\n",
      "11123577\n",
      "11123584\n",
      "11123592\n",
      "11123594\n",
      "11123598\n",
      "11123605\n",
      "11123611\n",
      "11123676\n",
      "11123688\n",
      "11123692\n",
      "11123759\n",
      "11123848\n",
      "11123851\n",
      "11123855\n",
      "11123939\n",
      "11123965\n",
      "11123974\n",
      "11123992\n",
      "11123999\n",
      "11124037\n",
      "11124071\n",
      "11124104\n",
      "11124154\n",
      "11124190\n",
      "11124192\n",
      "11124221\n",
      "11124234\n",
      "11124238\n",
      "11124239\n",
      "11124260\n",
      "11124262\n",
      "11124302\n",
      "11124352\n",
      "11124375\n",
      "11124402\n",
      "11124404\n",
      "11124444\n",
      "11124447\n",
      "11124454\n",
      "11124462\n",
      "11124464\n",
      "11124468\n",
      "11124475\n",
      "11124481\n",
      "11124546\n",
      "11124558\n",
      "11124562\n",
      "11124629\n",
      "11124718\n",
      "11124721\n",
      "11124725\n",
      "11124809\n",
      "11124835\n",
      "11124844\n",
      "11124862\n",
      "11124869\n",
      "11124907\n",
      "11124941\n",
      "11124974\n",
      "11125024\n",
      "11125060\n",
      "11125062\n",
      "11125091\n",
      "11125104\n",
      "11125108\n",
      "11125109\n",
      "11125130\n",
      "11125132\n",
      "11125172\n",
      "11125222\n",
      "11125245\n",
      "11125272\n",
      "11125274\n",
      "11125314\n",
      "11125317\n",
      "11125324\n",
      "11125332\n",
      "11125334\n",
      "11125338\n",
      "11125345\n",
      "11125351\n",
      "11125416\n",
      "11125428\n",
      "11125432\n",
      "11125499\n",
      "11125588\n",
      "11125591\n",
      "11125595\n",
      "11125679\n",
      "11125705\n",
      "11125714\n",
      "11125732\n",
      "11125739\n",
      "11125777\n",
      "11125811\n",
      "11125844\n",
      "11125894\n",
      "11125930\n",
      "11125932\n",
      "11125961\n",
      "11125974\n",
      "11125978\n",
      "11125979\n",
      "11126000\n",
      "11126002\n",
      "11126042\n",
      "11126092\n",
      "11126115\n",
      "11126142\n",
      "11126144\n",
      "11126184\n",
      "11126187\n",
      "11126194\n",
      "11126202\n",
      "11126204\n",
      "11126208\n",
      "11126215\n",
      "11126221\n",
      "11126286\n",
      "11126298\n",
      "11126302\n",
      "11126369\n",
      "11126458\n",
      "11126461\n",
      "11126465\n",
      "11126549\n",
      "11126575\n",
      "11126584\n",
      "11126602\n",
      "11126609\n",
      "11126647\n",
      "11126681\n",
      "11126714\n",
      "11126764\n",
      "11126800\n",
      "11126802\n",
      "11126831\n",
      "11126844\n",
      "11126848\n",
      "11126849\n",
      "11126870\n",
      "11126872\n",
      "11126912\n",
      "11126962\n",
      "11126985\n",
      "11127012\n",
      "11127014\n",
      "11127054\n",
      "11127057\n",
      "11127064\n",
      "11127072\n",
      "11127074\n",
      "11127078\n",
      "11127085\n",
      "11127091\n",
      "11127156\n",
      "11127168\n",
      "11127172\n",
      "11127239\n",
      "11127328\n",
      "11127331\n",
      "11127335\n",
      "11127419\n",
      "11127445\n",
      "11127454\n",
      "11127472\n",
      "11127479\n",
      "11127517\n",
      "11127551\n",
      "11127584\n",
      "11127634\n",
      "11127670\n",
      "11127672\n",
      "11127701\n",
      "11127714\n",
      "11127718\n",
      "11127719\n",
      "11127740\n",
      "11127742\n",
      "11127782\n",
      "11127832\n",
      "11127855\n",
      "11127882\n",
      "11127884\n",
      "11127924\n",
      "11127927\n",
      "11127934\n",
      "11127942\n",
      "11127944\n",
      "11127948\n",
      "11127955\n",
      "11127961\n",
      "11128026\n",
      "11128038\n",
      "11128042\n",
      "11128109\n",
      "11128198\n",
      "11128201\n",
      "11128205\n",
      "11128289\n",
      "11128315\n",
      "11128324\n",
      "11128342\n",
      "11128349\n",
      "11128387\n",
      "11128421\n",
      "11128454\n",
      "11128504\n",
      "11128540\n",
      "11128542\n",
      "11128571\n",
      "11128584\n",
      "11128588\n",
      "11128589\n",
      "11128610\n",
      "11128612\n",
      "11128652\n",
      "11128702\n",
      "11128725\n",
      "11128752\n",
      "11128754\n",
      "11128794\n",
      "11128797\n",
      "11128804\n",
      "11128812\n",
      "11128814\n",
      "11128818\n",
      "11128825\n",
      "11128831\n",
      "11128896\n",
      "11128908\n",
      "11128912\n",
      "11128979\n",
      "11129068\n",
      "11129071\n",
      "11129075\n",
      "11129159\n",
      "11129185\n",
      "11129194\n",
      "11129212\n",
      "11129219\n",
      "11129257\n",
      "11129291\n",
      "11129324\n",
      "11129374\n",
      "11129410\n",
      "11129412\n",
      "11129441\n",
      "11129454\n",
      "11129458\n",
      "11129459\n",
      "11129480\n",
      "11129482\n",
      "11129522\n",
      "11129572\n",
      "11129595\n",
      "11129622\n",
      "11129624\n",
      "11129664\n",
      "11129667\n",
      "11129674\n",
      "11129682\n",
      "11129684\n",
      "11129688\n",
      "11129695\n",
      "11129701\n",
      "11129766\n",
      "11129778\n",
      "11129782\n",
      "11129849\n",
      "11129938\n",
      "11129941\n",
      "11129945\n",
      "11130029\n",
      "11130055\n",
      "11130064\n",
      "11130082\n",
      "11130089\n",
      "11130127\n",
      "11130161\n",
      "11130194\n",
      "11130244\n",
      "11130280\n",
      "11130282\n",
      "11130311\n",
      "11130324\n",
      "11130328\n",
      "11130329\n",
      "11130350\n",
      "11130352\n",
      "11130392\n",
      "11130442\n",
      "11130465\n",
      "11130492\n",
      "11130494\n",
      "11130534\n",
      "11130537\n",
      "11130544\n",
      "11130552\n",
      "11130554\n",
      "11130558\n",
      "11130565\n",
      "11130571\n",
      "11130636\n",
      "11130648\n",
      "11130652\n",
      "11130719\n",
      "11130808\n",
      "11130811\n",
      "11130815\n",
      "11130899\n",
      "11130925\n",
      "11130934\n",
      "11130952\n",
      "11130959\n",
      "11130997\n",
      "11131031\n",
      "11131064\n",
      "11131114\n",
      "11131150\n",
      "11131152\n",
      "11131181\n",
      "11131194\n",
      "11131198\n",
      "11131199\n",
      "11131220\n",
      "11131222\n",
      "11131262\n",
      "11131312\n",
      "11131335\n",
      "11131362\n",
      "11131364\n",
      "11131404\n",
      "11131407\n",
      "11131414\n",
      "11131422\n",
      "11131424\n",
      "11131428\n",
      "11131435\n",
      "11131441\n",
      "11131506\n",
      "11131518\n",
      "11131522\n",
      "11131589\n",
      "11131678\n",
      "11131681\n",
      "11131685\n",
      "11131769\n",
      "11131795\n",
      "11131804\n",
      "11131822\n",
      "11131829\n",
      "11131867\n",
      "11131901\n",
      "11131934\n",
      "11131984\n",
      "11132020\n",
      "11132022\n",
      "11132051\n",
      "11132064\n",
      "11132068\n",
      "11132069\n",
      "11132090\n",
      "11132092\n",
      "11132132\n",
      "11132182\n",
      "11132205\n",
      "11132232\n",
      "11132234\n",
      "11132274\n",
      "11132277\n",
      "11132284\n",
      "11132292\n",
      "11132294\n",
      "11132298\n",
      "11132305\n",
      "11132311\n",
      "11132376\n",
      "11132388\n",
      "11132392\n",
      "11132459\n",
      "11132548\n",
      "11132551\n",
      "11132555\n",
      "11132639\n",
      "11132665\n",
      "11132674\n",
      "11132692\n",
      "11132699\n",
      "11132737\n",
      "11132771\n",
      "11132804\n",
      "11132854\n",
      "11132890\n",
      "11132892\n",
      "11132921\n",
      "11132934\n",
      "11132938\n",
      "11132939\n",
      "11132960\n",
      "11132962\n",
      "11133002\n",
      "11133052\n",
      "11133075\n",
      "11133102\n",
      "11133104\n",
      "11133144\n",
      "11133147\n",
      "11133154\n",
      "11133162\n",
      "11133164\n",
      "11133168\n",
      "11133175\n",
      "11133181\n",
      "11133246\n",
      "11133258\n",
      "11133262\n",
      "11133329\n",
      "11133418\n",
      "11133421\n",
      "11133425\n",
      "11133509\n",
      "11133535\n",
      "11133544\n",
      "11133562\n",
      "11133569\n",
      "11133607\n",
      "11133641\n",
      "11133674\n",
      "11133724\n",
      "11133760\n",
      "11133762\n",
      "11133791\n",
      "11133804\n",
      "11133808\n",
      "11133809\n",
      "11133830\n",
      "11133832\n",
      "11133872\n",
      "11133922\n",
      "11133945\n",
      "11133972\n",
      "11133974\n",
      "11134014\n",
      "11134017\n",
      "11134024\n",
      "11134032\n",
      "11134034\n",
      "11134038\n",
      "11134045\n",
      "11134051\n",
      "11134116\n",
      "11134128\n",
      "11134132\n",
      "11134199\n",
      "11134288\n",
      "11134291\n",
      "11134295\n",
      "11134379\n",
      "11134405\n",
      "11134414\n",
      "11134432\n",
      "11134439\n",
      "11134477\n",
      "11134511\n",
      "11134544\n",
      "11134594\n",
      "11134630\n",
      "11134632\n",
      "11134661\n",
      "11134674\n",
      "11134678\n",
      "11134679\n",
      "11134700\n",
      "11134702\n",
      "11134742\n",
      "11134792\n",
      "11134815\n",
      "11134842\n",
      "11134844\n",
      "11134884\n",
      "11134887\n",
      "11134894\n",
      "11134902\n",
      "11134904\n",
      "11134908\n",
      "11134915\n",
      "11134921\n",
      "11134986\n",
      "11134998\n",
      "11135002\n",
      "11135069\n",
      "11135158\n",
      "11135161\n",
      "11135165\n",
      "11135249\n",
      "11135275\n",
      "11135284\n",
      "11135302\n",
      "11135309\n",
      "11135347\n",
      "11135381\n",
      "11135414\n",
      "11135464\n",
      "11135500\n",
      "11135502\n",
      "11135531\n",
      "11135544\n",
      "11135548\n",
      "11135549\n",
      "11135570\n",
      "11135572\n",
      "11135612\n",
      "11135662\n",
      "11135685\n",
      "11135712\n",
      "11135714\n",
      "11135754\n",
      "11135757\n",
      "11135764\n",
      "11135772\n",
      "11135774\n",
      "11135778\n",
      "11135785\n",
      "11135791\n",
      "11135856\n",
      "11135868\n",
      "11135872\n",
      "11135939\n",
      "11136028\n",
      "11136031\n",
      "11136035\n",
      "11136119\n",
      "11136145\n",
      "11136154\n",
      "11136172\n",
      "11136179\n",
      "11136217\n",
      "11136251\n",
      "11136284\n",
      "11136334\n",
      "11136370\n",
      "11136372\n",
      "11136401\n",
      "11136414\n",
      "11136418\n",
      "11136419\n",
      "11136440\n",
      "11136442\n",
      "11136482\n",
      "11136532\n",
      "11136555\n",
      "11136582\n",
      "11136584\n",
      "11136624\n",
      "11136627\n",
      "11136634\n",
      "11136642\n",
      "11136644\n",
      "11136648\n",
      "11136655\n",
      "11136661\n",
      "11136726\n",
      "11136738\n",
      "11136742\n",
      "11136809\n",
      "11136898\n",
      "11136901\n",
      "11136905\n",
      "11136989\n",
      "11137015\n",
      "11137024\n",
      "11137042\n",
      "11137049\n",
      "11137087\n",
      "11137121\n",
      "11137154\n",
      "11137204\n",
      "11137240\n",
      "11137242\n",
      "11137271\n",
      "11137284\n",
      "11137288\n",
      "11137289\n",
      "11137310\n",
      "11137312\n",
      "11137352\n",
      "11137402\n",
      "11137425\n",
      "11137452\n",
      "11137454\n",
      "11137494\n",
      "11137497\n",
      "11137504\n",
      "11137512\n",
      "11137514\n",
      "11137518\n",
      "11137525\n",
      "11137531\n",
      "11137596\n",
      "11137608\n",
      "11137612\n",
      "11137679\n",
      "11137768\n",
      "11137771\n",
      "11137775\n",
      "11137859\n",
      "11137885\n",
      "11137894\n",
      "11137912\n",
      "11137919\n",
      "11137957\n",
      "11137991\n",
      "11138024\n",
      "11138074\n",
      "11138110\n",
      "11138112\n",
      "11138141\n",
      "11138154\n",
      "11138158\n",
      "11138159\n",
      "11138180\n",
      "11138182\n",
      "11138222\n",
      "11138272\n",
      "11138295\n",
      "11138322\n",
      "11138324\n",
      "11138364\n",
      "11138367\n",
      "11138374\n",
      "11138382\n",
      "11138384\n",
      "11138388\n",
      "11138395\n",
      "11138401\n",
      "11138466\n",
      "11138478\n",
      "11138482\n",
      "11138549\n",
      "11138638\n",
      "11138641\n",
      "11138645\n",
      "11138729\n",
      "11138755\n",
      "11138764\n",
      "11138782\n",
      "11138789\n",
      "11138827\n",
      "11138861\n",
      "11138894\n",
      "11138944\n",
      "11138980\n",
      "11138982\n",
      "11139011\n",
      "11139024\n",
      "11139028\n",
      "11139029\n",
      "11139050\n",
      "11139052\n",
      "11139092\n",
      "11139142\n",
      "11139165\n",
      "11139192\n",
      "11139194\n",
      "11139234\n",
      "11139237\n",
      "11139244\n",
      "11139252\n",
      "11139254\n",
      "11139258\n",
      "11139265\n",
      "11139271\n",
      "11139336\n",
      "11139348\n",
      "11139352\n",
      "11139419\n",
      "11139508\n",
      "11139511\n",
      "11139515\n",
      "11139599\n",
      "11139625\n",
      "11139634\n",
      "11139652\n",
      "11139659\n",
      "11139697\n",
      "11139731\n",
      "11139764\n",
      "11139814\n",
      "11139850\n",
      "11139852\n",
      "11139881\n",
      "11139894\n",
      "11139898\n",
      "11139899\n",
      "11139920\n",
      "11139922\n",
      "11139962\n",
      "11140012\n",
      "11140035\n",
      "11140062\n",
      "11140064\n",
      "11140104\n",
      "11140107\n",
      "11140114\n",
      "11140122\n",
      "11140124\n",
      "11140128\n",
      "11140135\n",
      "11140141\n",
      "11140206\n",
      "11140218\n",
      "11140222\n",
      "11140289\n",
      "11140378\n",
      "11140381\n",
      "11140385\n",
      "11140469\n",
      "11140495\n",
      "11140504\n",
      "11140522\n",
      "11140529\n",
      "11140567\n",
      "11140601\n",
      "11140634\n",
      "11140684\n",
      "11140720\n",
      "11140722\n",
      "11140751\n",
      "11140764\n",
      "11140768\n",
      "11140769\n",
      "11140790\n",
      "11140792\n",
      "11140832\n",
      "11140882\n",
      "11140905\n",
      "11140932\n",
      "11140934\n",
      "11140974\n",
      "11140977\n",
      "11140984\n",
      "11140992\n",
      "11140994\n",
      "11140998\n",
      "11141005\n",
      "11141011\n",
      "11141076\n",
      "11141088\n",
      "11141092\n",
      "11141159\n",
      "11141248\n",
      "11141251\n",
      "11141255\n",
      "11141339\n",
      "11141365\n",
      "11141374\n",
      "11141392\n",
      "11141399\n",
      "11141437\n",
      "11141471\n",
      "11141504\n",
      "11141554\n",
      "11141590\n",
      "11141592\n",
      "11141621\n",
      "11141634\n",
      "11141638\n",
      "11141639\n",
      "11141660\n",
      "11141662\n",
      "11141702\n",
      "11141752\n",
      "11141775\n",
      "11141802\n",
      "11141804\n",
      "11141844\n",
      "11141847\n",
      "11141854\n",
      "11141862\n",
      "11141864\n",
      "11141868\n",
      "11141875\n",
      "11141881\n",
      "11141946\n",
      "11141958\n",
      "11141962\n",
      "11142029\n",
      "11142118\n",
      "11142121\n",
      "11142125\n",
      "11142209\n",
      "11142235\n",
      "11142244\n",
      "11142262\n",
      "11142269\n",
      "11142307\n",
      "11142341\n",
      "11142374\n",
      "11142424\n",
      "11142460\n",
      "11142462\n",
      "11142491\n",
      "11142504\n",
      "11142508\n",
      "11142509\n",
      "11142530\n",
      "11142532\n",
      "11142572\n",
      "11142622\n",
      "11142645\n",
      "11142672\n",
      "11142674\n",
      "11142714\n",
      "11142717\n",
      "11142724\n",
      "11142732\n",
      "11142734\n",
      "11142738\n",
      "11142745\n",
      "11142751\n",
      "11142816\n",
      "11142828\n",
      "11142832\n",
      "11142899\n",
      "11142988\n",
      "11142991\n",
      "11142995\n",
      "11143079\n",
      "11143105\n",
      "11143114\n",
      "11143132\n",
      "11143139\n",
      "11143177\n",
      "11143211\n",
      "11143244\n",
      "11143294\n",
      "11143330\n",
      "11143332\n",
      "11143361\n",
      "11143374\n",
      "11143378\n",
      "11143379\n",
      "11143400\n",
      "11143402\n",
      "11143442\n",
      "11143492\n",
      "11143515\n",
      "11143542\n",
      "11143544\n",
      "11143584\n",
      "11143587\n",
      "11143594\n",
      "11143602\n",
      "11143604\n",
      "11143608\n",
      "11143615\n",
      "11143621\n",
      "11143686\n",
      "11143698\n",
      "11143702\n",
      "11143769\n",
      "11143858\n",
      "11143861\n",
      "11143865\n",
      "11143949\n",
      "11143975\n",
      "11143984\n",
      "11144002\n",
      "11144009\n",
      "11144047\n",
      "11144081\n",
      "11144114\n",
      "11144164\n",
      "11144200\n",
      "11144202\n",
      "11144231\n",
      "11144244\n",
      "11144248\n",
      "11144249\n",
      "11144270\n",
      "11144272\n",
      "11144312\n",
      "11144362\n",
      "11144385\n",
      "11144412\n",
      "11144414\n",
      "11144454\n",
      "11144457\n",
      "11144464\n",
      "11144472\n",
      "11144474\n",
      "11144478\n",
      "11144485\n",
      "11144491\n",
      "11144556\n",
      "11144568\n",
      "11144572\n",
      "11144639\n",
      "11144728\n",
      "11144731\n",
      "11144735\n",
      "11144819\n",
      "11144845\n",
      "11144854\n",
      "11144872\n",
      "11144879\n",
      "11144917\n",
      "11144951\n",
      "11144984\n",
      "11145034\n",
      "11145070\n",
      "11145072\n",
      "11145101\n",
      "11145114\n",
      "11145118\n",
      "11145119\n",
      "11145140\n",
      "11145142\n",
      "11145182\n",
      "11145232\n",
      "11145255\n",
      "11145282\n",
      "11145284\n",
      "11145324\n",
      "11145327\n",
      "11145334\n",
      "11145342\n",
      "11145344\n",
      "11145348\n",
      "11145355\n",
      "11145361\n",
      "11145426\n",
      "11145438\n",
      "11145442\n",
      "11145509\n",
      "11145598\n",
      "11145601\n",
      "11145605\n",
      "11145689\n",
      "11145715\n",
      "11145724\n",
      "11145742\n",
      "11145749\n",
      "11145787\n",
      "11145821\n",
      "11145854\n",
      "11145904\n",
      "11145940\n",
      "11145942\n",
      "11145971\n",
      "11145984\n",
      "11145988\n",
      "11145989\n",
      "11146010\n",
      "11146012\n",
      "11146052\n",
      "11146102\n",
      "11146125\n",
      "11146152\n",
      "11146154\n",
      "11146194\n",
      "11146197\n",
      "11146204\n",
      "11146212\n",
      "11146214\n",
      "11146218\n",
      "11146225\n",
      "11146231\n",
      "11146296\n",
      "11146308\n",
      "11146312\n",
      "11146379\n",
      "11146468\n",
      "11146471\n",
      "11146475\n",
      "11146559\n",
      "11146585\n",
      "11146594\n",
      "11146612\n",
      "11146619\n",
      "11146657\n",
      "11146691\n",
      "11146724\n",
      "11146774\n",
      "11146810\n",
      "11146812\n",
      "11146841\n",
      "11146854\n",
      "11146858\n",
      "11146859\n",
      "11146880\n",
      "11146882\n",
      "11146922\n",
      "11146972\n",
      "11146995\n",
      "11147022\n",
      "11147024\n",
      "11147064\n",
      "11147067\n",
      "11147074\n",
      "11147082\n",
      "11147084\n",
      "11147088\n",
      "11147095\n",
      "11147101\n",
      "11147166\n",
      "11147178\n",
      "11147182\n",
      "11147249\n",
      "11147338\n",
      "11147341\n",
      "11147345\n",
      "11147429\n",
      "11147455\n",
      "11147464\n",
      "11147482\n",
      "11147489\n",
      "11147527\n",
      "11147561\n",
      "11147594\n",
      "11147644\n",
      "11147680\n",
      "11147682\n",
      "11147711\n",
      "11147724\n",
      "11147728\n",
      "11147729\n",
      "11147750\n",
      "11147752\n",
      "11147792\n",
      "11147842\n",
      "11147865\n",
      "11147892\n",
      "11147894\n",
      "11147934\n",
      "11147937\n",
      "11147944\n",
      "11147952\n",
      "11147954\n",
      "11147958\n",
      "11147965\n",
      "11147971\n",
      "11148036\n",
      "11148048\n",
      "11148052\n",
      "11148119\n",
      "11148208\n",
      "11148211\n",
      "11148215\n",
      "11148299\n",
      "11148325\n",
      "11148334\n",
      "11148352\n",
      "11148359\n",
      "11148397\n",
      "11148431\n",
      "11148464\n",
      "11148514\n",
      "11148550\n",
      "11148552\n",
      "11148581\n",
      "11148594\n",
      "11148598\n",
      "11148599\n",
      "11148620\n",
      "11148622\n",
      "11148662\n",
      "11148712\n",
      "11148735\n",
      "11148762\n",
      "11148764\n",
      "11148804\n",
      "11148807\n",
      "11148814\n",
      "11148822\n",
      "11148824\n",
      "11148828\n",
      "11148835\n",
      "11148841\n",
      "11148906\n",
      "11148918\n",
      "11148922\n",
      "11148989\n",
      "11149078\n",
      "11149081\n",
      "11149085\n",
      "11149169\n",
      "11149195\n",
      "11149204\n",
      "11149222\n",
      "11149229\n",
      "11149267\n",
      "11149301\n",
      "11149334\n",
      "11149384\n",
      "11149420\n",
      "11149422\n",
      "11149451\n",
      "11149464\n",
      "11149468\n",
      "11149469\n",
      "11149490\n",
      "11149492\n",
      "11149532\n",
      "11149582\n",
      "11149605\n",
      "11149632\n",
      "11149634\n",
      "11149674\n",
      "11149677\n",
      "11149684\n",
      "11149692\n",
      "11149694\n",
      "11149698\n",
      "11149705\n",
      "11149711\n",
      "11149776\n",
      "11149788\n",
      "11149792\n",
      "11149859\n",
      "11149948\n",
      "11149951\n",
      "11149955\n",
      "11150039\n",
      "11150065\n",
      "11150074\n",
      "11150092\n",
      "11150099\n",
      "11150137\n",
      "11150171\n",
      "11150204\n",
      "11150254\n",
      "11150290\n",
      "11150292\n",
      "11150321\n",
      "11150334\n",
      "11150338\n",
      "11150339\n",
      "11150360\n",
      "11150362\n",
      "11150402\n",
      "11150452\n",
      "11150475\n",
      "11150502\n",
      "11150504\n",
      "11150544\n",
      "11150547\n",
      "11150554\n",
      "11150562\n",
      "11150564\n",
      "11150568\n",
      "11150575\n",
      "11150581\n",
      "11150646\n",
      "11150658\n",
      "11150662\n",
      "11150729\n",
      "11150818\n",
      "11150821\n",
      "11150825\n",
      "11150909\n",
      "11150935\n",
      "11150944\n",
      "11150962\n",
      "11150969\n",
      "11151007\n",
      "11151041\n",
      "11151074\n",
      "11151124\n",
      "11151160\n",
      "11151162\n",
      "11151191\n",
      "11151204\n",
      "11151208\n",
      "11151209\n",
      "11151230\n",
      "11151232\n",
      "11151272\n",
      "11151322\n",
      "11151345\n",
      "11151372\n",
      "11151374\n",
      "11151414\n",
      "11151417\n",
      "11151424\n",
      "11151432\n",
      "11151434\n",
      "11151438\n",
      "11151445\n",
      "11151451\n",
      "11151516\n",
      "11151528\n",
      "11151532\n",
      "11151599\n",
      "11151688\n",
      "11151691\n",
      "11151695\n",
      "11151779\n",
      "11151805\n",
      "11151814\n",
      "11151832\n",
      "11151839\n",
      "11151877\n",
      "11151911\n",
      "11151944\n",
      "11151994\n",
      "11152030\n",
      "11152032\n",
      "11152061\n",
      "11152074\n",
      "11152078\n",
      "11152079\n",
      "11152100\n",
      "11152102\n",
      "11152142\n",
      "11152192\n",
      "11152215\n",
      "11152242\n",
      "11152244\n",
      "11152284\n",
      "11152287\n",
      "11152294\n",
      "11152302\n",
      "11152304\n",
      "11152308\n",
      "11152315\n",
      "11152321\n",
      "11152386\n",
      "11152398\n",
      "11152402\n",
      "11152469\n",
      "11152558\n",
      "11152561\n",
      "11152565\n",
      "11152649\n",
      "11152675\n",
      "11152684\n",
      "11152702\n",
      "11152709\n",
      "11152747\n",
      "11152781\n",
      "11152814\n",
      "11152864\n",
      "11152900\n",
      "11152902\n",
      "11152931\n",
      "11152944\n",
      "11152948\n",
      "11152949\n",
      "11152970\n",
      "11152972\n",
      "11153012\n",
      "11153062\n",
      "11153085\n",
      "11153112\n",
      "11153114\n",
      "11153154\n",
      "11153157\n",
      "11153164\n",
      "11153172\n",
      "11153174\n",
      "11153178\n",
      "11153185\n",
      "11153191\n",
      "11153256\n",
      "11153268\n",
      "11153272\n",
      "11153339\n",
      "11153428\n",
      "11153431\n",
      "11153435\n",
      "11153519\n",
      "11153545\n",
      "11153554\n",
      "11153572\n",
      "11153579\n",
      "11153617\n",
      "11153651\n",
      "11153684\n",
      "11153734\n",
      "11153770\n",
      "11153772\n",
      "11153801\n",
      "11153814\n",
      "11153818\n",
      "11153819\n",
      "11153840\n",
      "11153842\n",
      "11153882\n",
      "11153932\n",
      "11153955\n",
      "11153982\n",
      "11153984\n",
      "11154024\n",
      "11154027\n",
      "11154034\n",
      "11154042\n",
      "11154044\n",
      "11154048\n",
      "11154055\n",
      "11154061\n",
      "11154126\n",
      "11154138\n",
      "11154142\n",
      "11154209\n",
      "11154298\n",
      "11154301\n",
      "11154305\n",
      "11154389\n",
      "11154415\n",
      "11154424\n",
      "11154442\n",
      "11154449\n",
      "11154487\n",
      "11154521\n",
      "11154554\n",
      "11154604\n",
      "11154640\n",
      "11154642\n",
      "11154671\n",
      "11154684\n",
      "11154688\n",
      "11154689\n",
      "11154710\n",
      "11154712\n",
      "11154752\n",
      "11154802\n",
      "11154825\n",
      "11154852\n",
      "11154854\n",
      "11154894\n",
      "11154897\n",
      "11154904\n",
      "11154912\n",
      "11154914\n",
      "11154918\n",
      "11154925\n",
      "11154931\n",
      "11154996\n",
      "11155008\n",
      "11155012\n",
      "11155079\n",
      "11155168\n",
      "11155171\n",
      "11155175\n",
      "11155259\n",
      "11155285\n",
      "11155294\n",
      "11155312\n",
      "11155319\n",
      "11155357\n",
      "11155391\n",
      "11155424\n",
      "11155474\n",
      "11155510\n",
      "11155512\n",
      "11155541\n",
      "11155554\n",
      "11155558\n",
      "11155559\n",
      "11155580\n",
      "11155582\n",
      "11155622\n",
      "11155672\n",
      "11155695\n",
      "11155722\n",
      "11155724\n",
      "11155764\n",
      "11155767\n",
      "11155774\n",
      "11155782\n",
      "11155784\n",
      "11155788\n",
      "11155795\n",
      "11155801\n",
      "11155866\n",
      "11155878\n",
      "11155882\n",
      "11155949\n",
      "11156038\n",
      "11156041\n",
      "11156045\n",
      "11156129\n",
      "11156155\n",
      "11156164\n",
      "11156182\n",
      "11156189\n",
      "11156227\n",
      "11156261\n",
      "11156294\n",
      "11156344\n",
      "11156380\n",
      "11156382\n",
      "11156411\n",
      "11156424\n",
      "11156428\n",
      "11156429\n",
      "11156450\n",
      "11156452\n",
      "11156492\n",
      "11156542\n",
      "11156565\n",
      "11156592\n",
      "11156594\n",
      "11156634\n",
      "11156637\n",
      "11156644\n",
      "11156652\n",
      "11156654\n",
      "11156658\n",
      "11156665\n",
      "11156671\n",
      "11156736\n",
      "11156748\n",
      "11156752\n",
      "11156819\n",
      "11156908\n",
      "11156911\n",
      "11156915\n",
      "11156999\n",
      "11157025\n",
      "11157034\n",
      "11157052\n",
      "11157059\n",
      "11157097\n",
      "11157131\n",
      "11157164\n",
      "11157214\n",
      "11157250\n",
      "11157252\n",
      "11157281\n",
      "11157294\n",
      "11157298\n",
      "11157299\n",
      "11157320\n",
      "11157322\n",
      "11157362\n",
      "11157412\n",
      "11157435\n",
      "11157462\n",
      "11157464\n",
      "11157504\n",
      "11157507\n",
      "11157514\n",
      "11157522\n",
      "11157524\n",
      "11157528\n",
      "11157535\n",
      "11157541\n",
      "11157606\n",
      "11157618\n",
      "11157622\n",
      "11157689\n",
      "11157778\n",
      "11157781\n",
      "11157785\n",
      "11157869\n",
      "11157895\n",
      "11157904\n",
      "11157922\n",
      "11157929\n",
      "11157967\n",
      "11158001\n",
      "11158034\n",
      "11158084\n",
      "11158120\n",
      "11158122\n",
      "11158151\n",
      "11158164\n",
      "11158168\n",
      "11158169\n",
      "11158190\n",
      "11158192\n",
      "11158232\n",
      "11158282\n",
      "11158305\n",
      "11158332\n",
      "11158334\n",
      "11158374\n",
      "11158377\n",
      "11158384\n",
      "11158392\n",
      "11158394\n",
      "11158398\n",
      "11158405\n",
      "11158411\n",
      "11158476\n",
      "11158488\n",
      "11158492\n",
      "11158559\n",
      "11158648\n",
      "11158651\n",
      "11158655\n",
      "11158739\n",
      "11158765\n",
      "11158774\n",
      "11158792\n",
      "11158799\n",
      "11158837\n",
      "11158871\n",
      "11158904\n",
      "11158954\n",
      "11158990\n",
      "11158992\n",
      "11159021\n",
      "11159034\n",
      "11159038\n",
      "11159039\n",
      "11159060\n",
      "11159062\n",
      "11159102\n",
      "11159152\n",
      "11159175\n",
      "11159202\n",
      "11159204\n",
      "11159244\n",
      "11159247\n",
      "11159254\n",
      "11159262\n",
      "11159264\n",
      "11159268\n",
      "11159275\n",
      "11159281\n",
      "11159346\n",
      "11159358\n",
      "11159362\n",
      "11159429\n",
      "11159518\n",
      "11159521\n",
      "11159525\n",
      "11159609\n",
      "11159635\n",
      "11159644\n",
      "11159662\n",
      "11159669\n",
      "11159707\n",
      "11159741\n",
      "11159774\n",
      "11159824\n",
      "11159860\n",
      "11159862\n",
      "11159891\n",
      "11159904\n",
      "11159908\n",
      "11159909\n",
      "11159930\n",
      "11159932\n",
      "11159972\n",
      "11160022\n",
      "11160045\n",
      "11160072\n",
      "11160074\n",
      "11160114\n",
      "11160117\n",
      "11160124\n",
      "11160132\n",
      "11160134\n",
      "11160138\n",
      "11160145\n",
      "11160151\n",
      "11160216\n",
      "11160228\n",
      "11160232\n",
      "11160299\n",
      "11160388\n",
      "11160391\n",
      "11160395\n",
      "11160479\n",
      "11160505\n",
      "11160514\n",
      "11160532\n",
      "11160539\n",
      "11160577\n",
      "11160611\n",
      "11160644\n",
      "11160694\n",
      "11160730\n",
      "11160732\n",
      "11160761\n",
      "11160774\n",
      "11160778\n",
      "11160779\n",
      "11160800\n",
      "11160802\n",
      "11160842\n",
      "11160892\n",
      "11160915\n",
      "11160942\n",
      "11160944\n",
      "11160984\n",
      "11160987\n",
      "11160994\n",
      "11161002\n",
      "11161004\n",
      "11161008\n",
      "11161015\n",
      "11161021\n",
      "11161086\n",
      "11161098\n",
      "11161102\n",
      "11161169\n",
      "11161258\n",
      "11161261\n",
      "11161265\n",
      "11161349\n",
      "11161375\n",
      "11161384\n",
      "11161402\n",
      "11161409\n",
      "11161447\n",
      "11161481\n",
      "11161514\n",
      "11161564\n",
      "11161600\n",
      "11161602\n",
      "11161631\n",
      "11161644\n",
      "11161648\n",
      "11161649\n",
      "11161670\n",
      "11161672\n",
      "11161712\n",
      "11161762\n",
      "11161785\n",
      "11161812\n",
      "11161814\n",
      "11161854\n",
      "11161857\n",
      "11161864\n",
      "11161872\n",
      "11161874\n",
      "11161878\n",
      "11161885\n",
      "11161891\n",
      "11161956\n",
      "11161968\n",
      "11161972\n",
      "11162039\n",
      "11162128\n",
      "11162131\n",
      "11162135\n",
      "11162219\n",
      "11162245\n",
      "11162254\n",
      "11162272\n",
      "11162279\n",
      "11162317\n",
      "11162351\n",
      "11162384\n",
      "11162434\n",
      "11162470\n",
      "11162472\n",
      "11162501\n",
      "11162514\n",
      "11162518\n",
      "11162519\n",
      "11162540\n",
      "11162542\n",
      "11162582\n",
      "11162632\n",
      "11162655\n",
      "11162682\n",
      "11162684\n",
      "11162724\n",
      "11162727\n",
      "11162734\n",
      "11162742\n",
      "11162744\n",
      "11162748\n",
      "11162755\n",
      "11162761\n",
      "11162826\n",
      "11162838\n",
      "11162842\n",
      "11162909\n",
      "11162998\n",
      "11163001\n",
      "11163005\n",
      "11163089\n",
      "11163115\n",
      "11163124\n",
      "11163142\n",
      "11163149\n",
      "11163187\n",
      "11163221\n",
      "11163254\n",
      "11163304\n",
      "11163340\n",
      "11163342\n",
      "11163371\n",
      "11163384\n",
      "11163388\n",
      "11163389\n",
      "11163410\n",
      "11163412\n",
      "11163452\n",
      "11163502\n",
      "11163525\n",
      "11163552\n",
      "11163554\n",
      "11163594\n",
      "11163597\n",
      "11163604\n",
      "11163612\n",
      "11163614\n",
      "11163618\n",
      "11163625\n",
      "11163631\n",
      "11163696\n",
      "11163708\n",
      "11163712\n",
      "11163779\n",
      "11163868\n",
      "11163871\n",
      "11163875\n",
      "11163959\n",
      "11163985\n",
      "11163994\n",
      "11164012\n",
      "11164019\n",
      "11164057\n",
      "11164091\n",
      "11164124\n",
      "11164174\n",
      "11164210\n",
      "11164212\n",
      "11164241\n",
      "11164254\n",
      "11164258\n",
      "11164259\n",
      "11164280\n",
      "11164282\n",
      "11164322\n",
      "11164372\n",
      "11164395\n",
      "11164422\n",
      "11164424\n",
      "11164464\n",
      "11164467\n",
      "11164474\n",
      "11164482\n",
      "11164484\n",
      "11164488\n",
      "11164495\n",
      "11164501\n",
      "11164566\n",
      "11164578\n",
      "11164582\n",
      "11164649\n",
      "11164738\n",
      "11164741\n",
      "11164745\n",
      "11164829\n",
      "11164855\n",
      "11164864\n",
      "11164882\n",
      "11164889\n",
      "11164927\n",
      "11164961\n",
      "11164994\n",
      "11165044\n",
      "11165080\n",
      "11165082\n",
      "11165111\n",
      "11165124\n",
      "11165128\n",
      "11165129\n",
      "11165150\n",
      "11165152\n",
      "11165192\n",
      "11165242\n",
      "11165265\n",
      "11165292\n",
      "11165294\n",
      "11165334\n",
      "11165337\n",
      "11165344\n",
      "11165352\n",
      "11165354\n",
      "11165358\n",
      "11165365\n",
      "11165371\n",
      "11165436\n",
      "11165448\n",
      "11165452\n",
      "11165519\n",
      "11165608\n",
      "11165611\n",
      "11165615\n",
      "11165699\n",
      "11165725\n",
      "11165734\n",
      "11165752\n",
      "11165759\n",
      "11165797\n",
      "11165831\n",
      "11165864\n",
      "11165914\n",
      "11165950\n",
      "11165952\n",
      "11165981\n",
      "11165994\n",
      "11165998\n",
      "11165999\n",
      "11166020\n",
      "11166022\n",
      "11166062\n",
      "11166112\n",
      "11166135\n",
      "11166162\n",
      "11166164\n",
      "11166204\n",
      "11166207\n",
      "11166214\n",
      "11166222\n",
      "11166224\n",
      "11166228\n",
      "11166235\n",
      "11166241\n",
      "11166306\n",
      "11166318\n",
      "11166322\n",
      "11166389\n",
      "11166478\n",
      "11166481\n",
      "11166485\n",
      "11166569\n",
      "11166595\n",
      "11166604\n",
      "11166622\n",
      "11166629\n",
      "11166667\n",
      "11166701\n",
      "11166734\n",
      "11166784\n",
      "11166820\n",
      "11166822\n",
      "11166851\n",
      "11166864\n",
      "11166868\n",
      "11166869\n",
      "11166890\n",
      "11166892\n",
      "11166932\n",
      "11166982\n",
      "11167005\n",
      "11167032\n",
      "11167034\n",
      "11167074\n",
      "11167077\n",
      "11167084\n",
      "11167092\n",
      "11167094\n",
      "11167098\n",
      "11167105\n",
      "11167111\n",
      "11167176\n",
      "11167188\n",
      "11167192\n",
      "11167259\n",
      "11167348\n",
      "11167351\n",
      "11167355\n",
      "11167439\n",
      "11167465\n",
      "11167474\n",
      "11167492\n",
      "11167499\n",
      "11167537\n",
      "11167571\n",
      "11167604\n",
      "11167654\n",
      "11167690\n",
      "11167692\n",
      "11167721\n",
      "11167734\n",
      "11167738\n",
      "11167739\n",
      "11167760\n",
      "11167762\n",
      "11167802\n",
      "11167852\n",
      "11167875\n",
      "11167902\n",
      "11167904\n",
      "11167944\n",
      "11167947\n",
      "11167954\n",
      "11167962\n",
      "11167964\n",
      "11167968\n",
      "11167975\n",
      "11167981\n",
      "11168046\n",
      "11168058\n",
      "11168062\n",
      "11168129\n",
      "11168218\n",
      "11168221\n",
      "11168225\n",
      "11168309\n",
      "11168335\n",
      "11168344\n",
      "11168362\n",
      "11168369\n",
      "11168407\n",
      "11168441\n",
      "11168474\n",
      "11168524\n",
      "11168560\n",
      "11168562\n",
      "11168591\n",
      "11168604\n",
      "11168608\n",
      "11168609\n",
      "11168630\n",
      "11168632\n",
      "11168672\n",
      "11168722\n",
      "11168745\n",
      "11168772\n",
      "11168774\n",
      "11168814\n",
      "11168817\n",
      "11168824\n",
      "11168832\n",
      "11168834\n",
      "11168838\n",
      "11168845\n",
      "11168851\n",
      "11168916\n",
      "11168928\n",
      "11168932\n",
      "11168999\n",
      "11169088\n",
      "11169091\n",
      "11169095\n",
      "11169179\n",
      "11169205\n",
      "11169214\n",
      "11169232\n",
      "11169239\n",
      "11169277\n",
      "11169311\n",
      "11169344\n",
      "11169394\n",
      "11169430\n",
      "11169432\n",
      "11169461\n",
      "11169474\n",
      "11169478\n",
      "11169479\n",
      "11169500\n",
      "11169502\n",
      "11169542\n",
      "11169592\n",
      "11169615\n",
      "11169642\n",
      "11169644\n",
      "11169684\n",
      "11169687\n",
      "11169694\n",
      "11169702\n",
      "11169704\n",
      "11169708\n",
      "11169715\n",
      "11169721\n",
      "11169786\n",
      "11169798\n",
      "11169802\n",
      "11169869\n",
      "11169958\n",
      "11169961\n",
      "11169965\n",
      "11170049\n",
      "11170075\n",
      "11170084\n",
      "11170102\n",
      "11170109\n",
      "11170147\n",
      "11170181\n",
      "11170214\n",
      "11170264\n",
      "11170300\n",
      "11170302\n",
      "11170331\n",
      "11170344\n",
      "11170348\n",
      "11170349\n",
      "11170370\n",
      "11170372\n",
      "11170412\n",
      "11170462\n",
      "11170485\n",
      "11170512\n",
      "11170514\n",
      "11170554\n",
      "11170557\n",
      "11170564\n",
      "11170572\n",
      "11170574\n",
      "11170578\n",
      "11170585\n",
      "11170591\n",
      "11170656\n",
      "11170668\n",
      "11170672\n",
      "11170739\n",
      "11170828\n",
      "11170831\n",
      "11170835\n",
      "11170919\n",
      "11170945\n",
      "11170954\n",
      "11170972\n",
      "11170979\n",
      "11171017\n",
      "11171051\n",
      "11171084\n",
      "11171134\n",
      "11171170\n",
      "11171172\n",
      "11171201\n",
      "11171214\n",
      "11171218\n",
      "11171219\n",
      "11171240\n",
      "11171242\n",
      "11171282\n",
      "11171332\n",
      "11171355\n",
      "11171382\n",
      "11171384\n",
      "11171424\n",
      "11171427\n",
      "11171434\n",
      "11171442\n",
      "11171444\n",
      "11171448\n",
      "11171455\n",
      "11171461\n",
      "11171526\n",
      "11171538\n",
      "11171542\n",
      "11171609\n",
      "11171698\n",
      "11171701\n",
      "11171705\n",
      "11171789\n",
      "11171815\n",
      "11171824\n",
      "11171842\n",
      "11171849\n",
      "11171887\n",
      "11171921\n",
      "11171954\n",
      "11172004\n",
      "11172040\n",
      "11172042\n",
      "11172071\n",
      "11172084\n",
      "11172088\n",
      "11172089\n",
      "11172110\n",
      "11172112\n",
      "11172152\n",
      "11172202\n",
      "11172225\n",
      "11172252\n",
      "11172254\n",
      "11172294\n",
      "11172297\n",
      "11172304\n",
      "11172312\n",
      "11172314\n",
      "11172318\n",
      "11172325\n",
      "11172331\n",
      "11172396\n",
      "11172408\n",
      "11172412\n",
      "11172479\n",
      "11172568\n",
      "11172571\n",
      "11172575\n",
      "11172659\n",
      "11172685\n",
      "11172694\n",
      "11172712\n",
      "11172719\n",
      "11172757\n",
      "11172791\n",
      "11172824\n",
      "11172874\n",
      "11172910\n",
      "11172912\n",
      "11172941\n",
      "11172954\n",
      "11172958\n",
      "11172959\n",
      "11172980\n",
      "11172982\n",
      "11173022\n",
      "11173072\n",
      "11173095\n",
      "11173122\n",
      "11173124\n",
      "11173164\n",
      "11173167\n",
      "11173174\n",
      "11173182\n",
      "11173184\n",
      "11173188\n",
      "11173195\n",
      "11173201\n",
      "11173266\n",
      "11173278\n",
      "11173282\n",
      "11173349\n",
      "11173438\n",
      "11173441\n",
      "11173445\n",
      "11173529\n",
      "11173555\n",
      "11173564\n",
      "11173582\n",
      "11173589\n",
      "11173627\n",
      "11173661\n",
      "11173694\n",
      "11173744\n",
      "11173780\n",
      "11173782\n",
      "11173811\n",
      "11173824\n",
      "11173828\n",
      "11173829\n",
      "11173850\n",
      "11173852\n",
      "11173892\n",
      "11173942\n",
      "11173965\n",
      "11173992\n",
      "11173994\n",
      "11174034\n",
      "11174037\n",
      "11174044\n",
      "11174052\n",
      "11174054\n",
      "11174058\n",
      "11174065\n",
      "11174071\n",
      "11174136\n",
      "11174148\n",
      "11174152\n",
      "11174219\n",
      "11174308\n",
      "11174311\n",
      "11174315\n",
      "11174399\n",
      "11174425\n",
      "11174434\n",
      "11174452\n",
      "11174459\n",
      "11174497\n",
      "11174531\n",
      "11174564\n",
      "11174614\n",
      "11174650\n",
      "11174652\n",
      "11174681\n",
      "11174694\n",
      "11174698\n",
      "11174699\n",
      "11174720\n",
      "11174722\n",
      "11174762\n",
      "11174812\n",
      "11174835\n",
      "11174862\n",
      "11174864\n",
      "11174904\n",
      "11174907\n",
      "11174914\n",
      "11174922\n",
      "11174924\n",
      "11174928\n",
      "11174935\n",
      "11174941\n",
      "11175006\n",
      "11175018\n",
      "11175022\n",
      "11175089\n",
      "11175178\n",
      "11175181\n",
      "11175185\n",
      "11175269\n",
      "11175295\n",
      "11175304\n",
      "11175322\n",
      "11175329\n",
      "11175367\n",
      "11175401\n",
      "11175434\n",
      "11175484\n",
      "11175520\n",
      "11175522\n",
      "11175551\n",
      "11175564\n",
      "11175568\n",
      "11175569\n",
      "11175590\n",
      "11175592\n",
      "11175632\n",
      "11175682\n",
      "11175705\n",
      "11175732\n",
      "11175734\n",
      "11175774\n",
      "11175777\n",
      "11175784\n",
      "11175792\n",
      "11175794\n",
      "11175798\n",
      "11175805\n",
      "11175811\n",
      "11175876\n",
      "11175888\n",
      "11175892\n",
      "11175959\n",
      "11176048\n",
      "11176051\n",
      "11176055\n",
      "11176139\n",
      "11176165\n",
      "11176174\n",
      "11176192\n",
      "11176199\n",
      "11176237\n",
      "11176271\n",
      "11176304\n",
      "11176354\n",
      "11176390\n",
      "11176392\n",
      "11176421\n",
      "11176434\n",
      "11176438\n",
      "11176439\n",
      "11176460\n",
      "11176462\n",
      "11176502\n",
      "11176552\n",
      "11176575\n",
      "11176602\n",
      "11176604\n",
      "11176644\n",
      "11176647\n",
      "11176654\n",
      "11176662\n",
      "11176664\n",
      "11176668\n",
      "11176675\n",
      "11176681\n",
      "11176746\n",
      "11176758\n",
      "11176762\n",
      "11176829\n",
      "11176918\n",
      "11176921\n",
      "11176925\n",
      "11177009\n",
      "11177035\n",
      "11177044\n",
      "11177062\n",
      "11177069\n",
      "11177107\n",
      "11177141\n",
      "11177174\n",
      "11177224\n",
      "11177260\n",
      "11177262\n",
      "11177291\n",
      "11177304\n",
      "11177308\n",
      "11177309\n",
      "11177330\n",
      "11177332\n",
      "11177372\n",
      "11177422\n",
      "11177445\n",
      "11177472\n",
      "11177474\n",
      "11177514\n",
      "11177517\n",
      "11177524\n",
      "11177532\n",
      "11177534\n",
      "11177538\n",
      "11177545\n",
      "11177551\n",
      "11177616\n",
      "11177628\n",
      "11177632\n",
      "11177699\n",
      "11177788\n",
      "11177791\n",
      "11177795\n",
      "11177879\n",
      "11177905\n",
      "11177914\n",
      "11177932\n",
      "11177939\n",
      "11177977\n",
      "11178011\n",
      "11178044\n",
      "11178094\n",
      "11178130\n",
      "11178132\n",
      "11178161\n",
      "11178174\n",
      "11178178\n",
      "11178179\n",
      "11178200\n",
      "11178202\n",
      "11178242\n",
      "11178292\n",
      "11178315\n",
      "11178342\n",
      "11178344\n",
      "11178384\n",
      "11178387\n",
      "11178394\n",
      "11178402\n",
      "11178404\n",
      "11178408\n",
      "11178415\n",
      "11178421\n",
      "11178486\n",
      "11178498\n",
      "11178502\n",
      "11178569\n",
      "11178658\n",
      "11178661\n",
      "11178665\n",
      "11178749\n",
      "11178775\n",
      "11178784\n",
      "11178802\n",
      "11178809\n",
      "11178847\n",
      "11178881\n",
      "11178914\n",
      "11178964\n",
      "11179000\n",
      "11179002\n",
      "11179031\n",
      "11179044\n",
      "11179048\n",
      "11179049\n",
      "11179070\n",
      "11179072\n",
      "11179112\n",
      "11179162\n",
      "11179185\n",
      "11179212\n",
      "11179214\n",
      "11179254\n",
      "11179257\n",
      "11179264\n",
      "11179272\n",
      "11179274\n",
      "11179278\n",
      "11179285\n",
      "11179291\n",
      "11179356\n",
      "11179368\n",
      "11179372\n",
      "11179439\n",
      "11179528\n",
      "11179531\n",
      "11179535\n",
      "11179619\n",
      "11179645\n",
      "11179654\n",
      "11179672\n",
      "11179679\n",
      "11179717\n",
      "11179751\n",
      "11179784\n",
      "11179834\n",
      "11179870\n",
      "11179872\n",
      "11179901\n",
      "11179914\n",
      "11179918\n",
      "11179919\n",
      "11179940\n",
      "11179942\n",
      "11179982\n",
      "11180032\n",
      "11180055\n",
      "11180082\n",
      "11180084\n",
      "11180124\n",
      "11180127\n",
      "11180134\n",
      "11180142\n",
      "11180144\n",
      "11180148\n",
      "11180155\n",
      "11180161\n",
      "11180226\n",
      "11180238\n",
      "11180242\n",
      "11180309\n",
      "11180398\n",
      "11180401\n",
      "11180405\n",
      "11180489\n",
      "11180515\n",
      "11180524\n",
      "11180542\n",
      "11180549\n",
      "11180587\n",
      "11180621\n",
      "11180654\n",
      "11180704\n",
      "11180740\n",
      "11180742\n",
      "11180771\n",
      "11180784\n",
      "11180788\n",
      "11180789\n",
      "11180810\n",
      "11180812\n",
      "11180852\n",
      "11180902\n",
      "11180925\n",
      "11180952\n",
      "11180954\n",
      "11180994\n",
      "11180997\n",
      "11181004\n",
      "11181012\n",
      "11181014\n",
      "11181018\n",
      "11181025\n",
      "11181031\n",
      "11181096\n",
      "11181108\n",
      "11181112\n",
      "11181179\n",
      "11181268\n",
      "11181271\n",
      "11181275\n",
      "11181359\n",
      "11181385\n",
      "11181394\n",
      "11181412\n",
      "11181419\n",
      "11181457\n",
      "11181491\n",
      "11181524\n",
      "11181574\n",
      "11181610\n",
      "11181612\n",
      "11181641\n",
      "11181654\n",
      "11181658\n",
      "11181659\n",
      "11181680\n",
      "11181682\n",
      "11181722\n",
      "11181772\n",
      "11181795\n",
      "11181822\n",
      "11181824\n",
      "11181864\n",
      "11181867\n",
      "11181874\n",
      "11181882\n",
      "11181884\n",
      "11181888\n",
      "11181895\n",
      "11181901\n",
      "11181966\n",
      "11181978\n",
      "11181982\n",
      "11182049\n",
      "11182138\n",
      "11182141\n",
      "11182145\n",
      "11182229\n",
      "11182255\n",
      "11182264\n",
      "11182282\n",
      "11182289\n",
      "11182327\n",
      "11182361\n",
      "11182394\n",
      "11182444\n",
      "11182480\n",
      "11182482\n",
      "11182511\n",
      "11182524\n",
      "11182528\n",
      "11182529\n",
      "11182550\n",
      "11182552\n",
      "11182592\n",
      "11182642\n",
      "11182665\n",
      "11182692\n",
      "11182694\n",
      "11182734\n",
      "11182737\n",
      "11182744\n",
      "11182752\n",
      "11182754\n",
      "11182758\n",
      "11182765\n",
      "11182771\n",
      "11182836\n",
      "11182848\n",
      "11182852\n",
      "11182919\n",
      "11183008\n",
      "11183011\n",
      "11183015\n",
      "11183099\n",
      "11183125\n",
      "11183134\n",
      "11183152\n",
      "11183159\n",
      "11183197\n",
      "11183231\n",
      "11183264\n",
      "11183314\n",
      "11183350\n",
      "11183352\n",
      "11183381\n",
      "11183394\n",
      "11183398\n",
      "11183399\n",
      "11183420\n",
      "11183422\n",
      "11183462\n",
      "11183512\n",
      "11183535\n",
      "11183562\n",
      "11183564\n",
      "11183604\n",
      "11183607\n",
      "11183614\n",
      "11183622\n",
      "11183624\n",
      "11183628\n",
      "11183635\n",
      "11183641\n",
      "11183706\n",
      "11183718\n",
      "11183722\n",
      "11183789\n",
      "11183878\n",
      "11183881\n",
      "11183885\n",
      "11183969\n",
      "11183995\n",
      "11184004\n",
      "11184022\n",
      "11184029\n",
      "11184067\n",
      "11184101\n",
      "11184134\n",
      "11184184\n",
      "11184220\n",
      "11184222\n",
      "11184251\n",
      "11184264\n",
      "11184268\n",
      "11184269\n",
      "11184290\n",
      "11184292\n",
      "11184332\n",
      "11184382\n",
      "11184405\n",
      "11184432\n",
      "11184434\n",
      "11184474\n",
      "11184477\n",
      "11184484\n",
      "11184492\n",
      "11184494\n",
      "11184498\n",
      "11184505\n",
      "11184511\n",
      "11184576\n",
      "11184588\n",
      "11184592\n",
      "11184659\n",
      "11184748\n",
      "11184751\n",
      "11184755\n",
      "11184839\n",
      "11184865\n",
      "11184874\n",
      "11184892\n",
      "11184899\n",
      "11184937\n",
      "11184971\n",
      "11185004\n",
      "11185054\n",
      "11185090\n",
      "11185092\n",
      "11185121\n",
      "11185134\n",
      "11185138\n",
      "11185139\n",
      "11185160\n",
      "11185162\n",
      "11185202\n",
      "11185252\n",
      "11185275\n",
      "11185302\n",
      "11185304\n",
      "11185344\n",
      "11185347\n",
      "11185354\n",
      "11185362\n",
      "11185364\n",
      "11185368\n",
      "11185375\n",
      "11185381\n",
      "11185446\n",
      "11185458\n",
      "11185462\n",
      "11185529\n",
      "11185618\n",
      "11185621\n",
      "11185625\n",
      "11185709\n",
      "11185735\n",
      "11185744\n",
      "11185762\n",
      "11185769\n",
      "11185807\n",
      "11185841\n",
      "11185874\n",
      "11185924\n",
      "11185960\n",
      "11185962\n",
      "11185991\n",
      "11186004\n",
      "11186008\n",
      "11186009\n",
      "11186030\n",
      "11186032\n",
      "11186072\n",
      "11186122\n",
      "11186145\n",
      "11186172\n",
      "11186174\n",
      "11186214\n",
      "11186217\n",
      "11186224\n",
      "11186232\n",
      "11186234\n",
      "11186238\n",
      "11186245\n",
      "11186251\n",
      "11186316\n",
      "11186328\n",
      "11186332\n",
      "11186399\n",
      "11186488\n",
      "11186491\n",
      "11186495\n",
      "11186579\n",
      "11186605\n",
      "11186614\n",
      "11186632\n",
      "11186639\n",
      "11186677\n",
      "11186711\n",
      "11186744\n",
      "11186794\n",
      "11186830\n",
      "11186832\n",
      "11186861\n",
      "11186874\n",
      "11186878\n",
      "11186879\n",
      "11186900\n",
      "11186902\n",
      "11186942\n",
      "11186992\n",
      "11187015\n",
      "11187042\n",
      "11187044\n",
      "11187084\n",
      "11187087\n",
      "11187094\n",
      "11187102\n",
      "11187104\n",
      "11187108\n",
      "11187115\n",
      "11187121\n",
      "11187186\n",
      "11187198\n",
      "11187202\n",
      "11187269\n",
      "11187358\n",
      "11187361\n",
      "11187365\n",
      "11187449\n",
      "11187475\n",
      "11187484\n",
      "11187502\n",
      "11187509\n",
      "11187547\n",
      "11187581\n",
      "11187614\n",
      "11187664\n",
      "11187700\n",
      "11187702\n",
      "11187731\n",
      "11187744\n",
      "11187748\n",
      "11187749\n",
      "11187770\n",
      "11187772\n",
      "11187812\n",
      "11187862\n",
      "11187885\n",
      "11187912\n",
      "11187914\n",
      "11187954\n",
      "11187957\n",
      "11187964\n",
      "11187972\n",
      "11187974\n",
      "11187978\n",
      "11187985\n",
      "11187991\n",
      "11188056\n",
      "11188068\n",
      "11188072\n",
      "11188139\n",
      "11188228\n",
      "11188231\n",
      "11188235\n",
      "11188319\n",
      "11188345\n",
      "11188354\n",
      "11188372\n",
      "11188379\n",
      "11188417\n",
      "11188451\n",
      "11188484\n",
      "11188534\n",
      "11188570\n",
      "11188572\n",
      "11188601\n",
      "11188614\n",
      "11188618\n",
      "11188619\n",
      "11188640\n",
      "11188642\n",
      "11188682\n",
      "11188732\n",
      "11188755\n",
      "11188782\n",
      "11188784\n",
      "11188824\n",
      "11188827\n",
      "11188834\n",
      "11188842\n",
      "11188844\n",
      "11188848\n",
      "11188855\n",
      "11188861\n",
      "11188926\n",
      "11188938\n",
      "11188942\n",
      "11189009\n",
      "11189098\n",
      "11189101\n",
      "11189105\n",
      "11189189\n",
      "11189215\n",
      "11189224\n",
      "11189242\n",
      "11189249\n",
      "11189287\n",
      "11189321\n",
      "11189354\n",
      "11189404\n",
      "11189440\n",
      "11189442\n",
      "11189471\n",
      "11189484\n",
      "11189488\n",
      "11189489\n",
      "11189510\n",
      "11189512\n",
      "11189552\n",
      "11189602\n",
      "11189625\n",
      "11189652\n",
      "11189654\n",
      "11189694\n",
      "11189697\n",
      "11189704\n",
      "11189712\n",
      "11189714\n",
      "11189718\n",
      "11189725\n",
      "11189731\n",
      "11189796\n",
      "11189808\n",
      "11189812\n",
      "11189879\n",
      "11189968\n",
      "11189971\n",
      "11189975\n",
      "11190059\n",
      "11190085\n",
      "11190094\n",
      "11190112\n",
      "11190119\n",
      "11190157\n",
      "11190191\n",
      "11190224\n",
      "11190274\n",
      "11190310\n",
      "11190312\n",
      "11190341\n",
      "11190354\n",
      "11190358\n",
      "11190359\n",
      "11190380\n",
      "11190382\n",
      "11190422\n",
      "11190472\n",
      "11190495\n",
      "11190522\n",
      "11190524\n",
      "11190564\n",
      "11190567\n",
      "11190574\n",
      "11190582\n",
      "11190584\n",
      "11190588\n",
      "11190595\n",
      "11190601\n",
      "11190666\n",
      "11190678\n",
      "11190682\n",
      "11190749\n",
      "11190838\n",
      "11190841\n",
      "11190845\n",
      "11190929\n",
      "11190955\n",
      "11190964\n",
      "11190982\n",
      "11190989\n",
      "11191027\n",
      "11191061\n",
      "11191094\n",
      "11191144\n",
      "11191180\n",
      "11191182\n",
      "11191211\n",
      "11191224\n",
      "11191228\n",
      "11191229\n",
      "11191250\n",
      "11191252\n",
      "11191292\n",
      "11191342\n",
      "11191365\n",
      "11191392\n",
      "11191394\n",
      "11191434\n",
      "11191437\n",
      "11191444\n",
      "11191452\n",
      "11191454\n",
      "11191458\n",
      "11191465\n",
      "11191471\n",
      "11191536\n",
      "11191548\n",
      "11191552\n",
      "11191619\n",
      "11191708\n",
      "11191711\n",
      "11191715\n",
      "11191799\n",
      "11191825\n",
      "11191834\n",
      "11191852\n",
      "11191859\n",
      "11191897\n",
      "11191931\n",
      "11191964\n",
      "11192014\n",
      "11192050\n",
      "11192052\n",
      "11192081\n",
      "11192094\n",
      "11192098\n",
      "11192099\n",
      "11192120\n",
      "11192122\n",
      "11192162\n",
      "11192212\n",
      "11192235\n",
      "11192262\n",
      "11192264\n",
      "11192304\n",
      "11192307\n",
      "11192314\n",
      "11192322\n",
      "11192324\n",
      "11192328\n",
      "11192335\n",
      "11192341\n",
      "11192406\n",
      "11192418\n",
      "11192422\n",
      "11192489\n",
      "11192578\n",
      "11192581\n",
      "11192585\n",
      "11192669\n",
      "11192695\n",
      "11192704\n",
      "11192722\n",
      "11192729\n",
      "11192767\n",
      "11192801\n",
      "11192834\n",
      "11192884\n",
      "11192920\n",
      "11192922\n",
      "11192951\n",
      "11192964\n",
      "11192968\n",
      "11192969\n",
      "11192990\n",
      "11192992\n",
      "11193032\n",
      "11193082\n",
      "11193105\n",
      "11193132\n",
      "11193134\n",
      "11193174\n",
      "11193177\n",
      "11193184\n",
      "11193192\n",
      "11193194\n",
      "11193198\n",
      "11193205\n",
      "11193211\n",
      "11193276\n",
      "11193288\n",
      "11193292\n",
      "11193359\n",
      "11193448\n",
      "11193451\n",
      "11193455\n",
      "11193539\n",
      "11193565\n",
      "11193574\n",
      "11193592\n",
      "11193599\n",
      "11193637\n",
      "11193671\n",
      "11193704\n",
      "11193754\n",
      "11193790\n",
      "11193792\n",
      "11193821\n",
      "11193834\n",
      "11193838\n",
      "11193839\n",
      "11193860\n",
      "11193862\n",
      "11193902\n",
      "11193952\n",
      "11193975\n",
      "11194002\n",
      "11194004\n",
      "11194044\n",
      "11194047\n",
      "11194054\n",
      "11194062\n",
      "11194064\n",
      "11194068\n",
      "11194075\n",
      "11194081\n",
      "11194146\n",
      "11194158\n",
      "11194162\n",
      "11194229\n",
      "11194318\n",
      "11194321\n",
      "11194325\n",
      "11194409\n",
      "11194435\n",
      "11194444\n",
      "11194462\n",
      "11194469\n",
      "11194507\n",
      "11194541\n",
      "11194574\n",
      "11194624\n",
      "11194660\n",
      "11194662\n",
      "11194691\n",
      "11194704\n",
      "11194708\n",
      "11194709\n",
      "11194730\n",
      "11194732\n",
      "11194772\n",
      "11194822\n",
      "11194845\n",
      "11194872\n",
      "11194874\n",
      "11194914\n",
      "11194917\n",
      "11194924\n",
      "11194932\n",
      "11194934\n",
      "11194938\n",
      "11194945\n",
      "11194951\n",
      "11195016\n",
      "11195028\n",
      "11195032\n",
      "11195099\n",
      "11195188\n",
      "11195191\n",
      "11195195\n",
      "11195279\n",
      "11195305\n",
      "11195314\n",
      "11195332\n",
      "11195339\n",
      "11195377\n",
      "11195411\n",
      "11195444\n",
      "11195494\n",
      "11195530\n",
      "11195532\n",
      "11195561\n",
      "11195574\n",
      "11195578\n",
      "11195579\n",
      "11195600\n",
      "11195602\n",
      "11195642\n",
      "11195692\n",
      "11195715\n",
      "11195742\n",
      "11195744\n",
      "11195784\n",
      "11195787\n",
      "11195794\n",
      "11195802\n",
      "11195804\n",
      "11195808\n",
      "11195815\n",
      "11195821\n",
      "11195886\n",
      "11195898\n",
      "11195902\n",
      "11195969\n",
      "11196058\n",
      "11196061\n",
      "11196065\n",
      "11196149\n",
      "11196175\n",
      "11196184\n",
      "11196202\n",
      "11196209\n",
      "11196247\n",
      "11196281\n",
      "11196314\n",
      "11196364\n",
      "11196400\n",
      "11196402\n",
      "11196431\n",
      "11196444\n",
      "11196448\n",
      "11196449\n",
      "11196470\n",
      "11196472\n",
      "11196512\n",
      "11196562\n",
      "11196585\n",
      "11196612\n",
      "11196614\n",
      "11196654\n",
      "11196657\n",
      "11196664\n",
      "11196672\n",
      "11196674\n",
      "11196678\n",
      "11196685\n",
      "11196691\n",
      "11196756\n",
      "11196768\n",
      "11196772\n",
      "11196839\n",
      "11196928\n",
      "11196931\n",
      "11196935\n",
      "11197019\n",
      "11197045\n",
      "11197054\n",
      "11197072\n",
      "11197079\n",
      "11197117\n",
      "11197151\n",
      "11197184\n",
      "11197234\n",
      "11197270\n",
      "11197272\n",
      "11197301\n",
      "11197314\n",
      "11197318\n",
      "11197319\n",
      "11197340\n",
      "11197342\n",
      "11197382\n",
      "11197432\n",
      "11197455\n",
      "11197482\n",
      "11197484\n",
      "11197524\n",
      "11197527\n",
      "11197534\n",
      "11197542\n",
      "11197544\n",
      "11197548\n",
      "11197555\n",
      "11197561\n",
      "11197626\n",
      "11197638\n",
      "11197642\n",
      "11197709\n",
      "11197798\n",
      "11197801\n",
      "11197805\n",
      "11197889\n",
      "11197915\n",
      "11197924\n",
      "11197942\n",
      "11197949\n",
      "11197987\n",
      "11198021\n",
      "11198054\n",
      "11198104\n",
      "11198140\n",
      "11198142\n",
      "11198171\n",
      "11198184\n",
      "11198188\n",
      "11198189\n",
      "11198210\n",
      "11198212\n",
      "11198252\n",
      "11198302\n",
      "11198325\n",
      "11198352\n",
      "11198354\n",
      "11198394\n",
      "11198397\n",
      "11198404\n",
      "11198412\n",
      "11198414\n",
      "11198418\n",
      "11198425\n",
      "11198431\n",
      "11198496\n",
      "11198508\n",
      "11198512\n",
      "11198579\n",
      "11198668\n",
      "11198671\n",
      "11198675\n",
      "11198759\n",
      "11198785\n",
      "11198794\n",
      "11198812\n",
      "11198819\n",
      "11198857\n",
      "11198891\n",
      "11198924\n",
      "11198974\n",
      "11199010\n",
      "11199012\n",
      "11199041\n",
      "11199054\n",
      "11199058\n",
      "11199059\n",
      "11199080\n",
      "11199082\n",
      "11199122\n",
      "11199172\n",
      "11199195\n",
      "11199222\n",
      "11199224\n",
      "11199264\n",
      "11199267\n",
      "11199274\n",
      "11199282\n",
      "11199284\n",
      "11199288\n",
      "11199295\n",
      "11199301\n",
      "11199366\n",
      "11199378\n",
      "11199382\n",
      "11199449\n",
      "11199538\n",
      "11199541\n",
      "11199545\n",
      "11199629\n",
      "11199655\n",
      "11199664\n",
      "11199682\n",
      "11199689\n",
      "11199727\n",
      "11199761\n",
      "11199794\n",
      "11199844\n",
      "11199880\n",
      "11199882\n",
      "11199911\n",
      "11199924\n",
      "11199928\n",
      "11199929\n",
      "11199950\n",
      "11199952\n",
      "11199992\n",
      "11200042\n",
      "11200065\n",
      "11200092\n",
      "11200094\n",
      "11200134\n",
      "11200137\n",
      "11200144\n",
      "11200152\n",
      "11200154\n",
      "11200158\n",
      "11200165\n",
      "11200171\n",
      "11200236\n",
      "11200248\n",
      "11200252\n",
      "11200319\n",
      "11200408\n",
      "11200411\n",
      "11200415\n",
      "11200499\n",
      "11200525\n",
      "11200534\n",
      "11200552\n",
      "11200559\n",
      "11200597\n",
      "11200631\n",
      "11200664\n",
      "11200714\n",
      "11200750\n",
      "11200752\n",
      "11200781\n",
      "11200794\n",
      "11200798\n",
      "11200799\n",
      "11200820\n",
      "11200822\n",
      "11200862\n",
      "11200912\n",
      "11200935\n",
      "11200962\n",
      "11200964\n",
      "11201004\n",
      "11201007\n",
      "11201014\n",
      "11201022\n",
      "11201024\n",
      "11201028\n",
      "11201035\n",
      "11201041\n",
      "11201106\n",
      "11201118\n",
      "11201122\n",
      "11201189\n",
      "11201278\n",
      "11201281\n",
      "11201285\n",
      "11201369\n",
      "11201395\n",
      "11201404\n",
      "11201422\n",
      "11201429\n",
      "11201467\n",
      "11201501\n",
      "11201534\n",
      "11201584\n",
      "11201620\n",
      "11201622\n",
      "11201651\n",
      "11201664\n",
      "11201668\n",
      "11201669\n",
      "11201690\n",
      "11201692\n",
      "11201732\n",
      "11201782\n",
      "11201805\n",
      "11201832\n",
      "11201834\n",
      "11201874\n",
      "11201877\n",
      "11201884\n",
      "11201892\n",
      "11201894\n",
      "11201898\n",
      "11201905\n",
      "11201911\n",
      "11201976\n",
      "11201988\n",
      "11201992\n",
      "11202059\n",
      "11202148\n",
      "11202151\n",
      "11202155\n",
      "11202239\n",
      "11202265\n",
      "11202274\n",
      "11202292\n",
      "11202299\n",
      "11202337\n",
      "11202371\n",
      "11202404\n",
      "11202454\n",
      "11202490\n",
      "11202492\n",
      "11202521\n",
      "11202534\n",
      "11202538\n",
      "11202539\n",
      "11202560\n",
      "11202562\n",
      "11202602\n",
      "11202652\n",
      "11202675\n",
      "11202702\n",
      "11202704\n",
      "11202744\n",
      "11202747\n",
      "11202754\n",
      "11202762\n",
      "11202764\n",
      "11202768\n",
      "11202775\n",
      "11202781\n",
      "11202846\n",
      "11202858\n",
      "11202862\n",
      "11202929\n",
      "11203018\n",
      "11203021\n",
      "11203025\n",
      "11203109\n",
      "11203135\n",
      "11203144\n",
      "11203162\n",
      "11203169\n",
      "11203207\n",
      "11203241\n",
      "11203274\n",
      "11203324\n",
      "11203360\n",
      "11203362\n",
      "11203391\n",
      "11203404\n",
      "11203408\n",
      "11203409\n",
      "11203430\n",
      "11203432\n",
      "11203472\n",
      "11203522\n",
      "11203545\n",
      "11203572\n",
      "11203574\n",
      "11203614\n",
      "11203617\n",
      "11203624\n",
      "11203632\n",
      "11203634\n",
      "11203638\n",
      "11203645\n",
      "11203651\n",
      "11203716\n",
      "11203728\n",
      "11203732\n",
      "11203799\n",
      "11203888\n",
      "11203891\n",
      "11203895\n",
      "11203979\n",
      "11204005\n",
      "11204014\n",
      "11204032\n",
      "11204039\n",
      "11204077\n",
      "11204111\n",
      "11204144\n",
      "11204194\n",
      "11204230\n",
      "11204232\n",
      "11204261\n",
      "11204274\n",
      "11204278\n",
      "11204279\n",
      "11204300\n",
      "11204302\n",
      "11204342\n",
      "11204392\n",
      "11204415\n",
      "11204442\n",
      "11204444\n",
      "11204484\n",
      "11204487\n",
      "11204494\n",
      "11204502\n",
      "11204504\n",
      "11204508\n",
      "11204515\n",
      "11204521\n",
      "11204586\n",
      "11204598\n",
      "11204602\n",
      "11204669\n",
      "11204758\n",
      "11204761\n",
      "11204765\n",
      "11204849\n",
      "11204875\n",
      "11204884\n",
      "11204902\n",
      "11204909\n",
      "11204947\n",
      "11204981\n",
      "11205014\n",
      "11205064\n",
      "11205100\n",
      "11205102\n",
      "11205131\n",
      "11205144\n",
      "11205148\n",
      "11205149\n",
      "11205170\n",
      "11205172\n",
      "11205212\n",
      "11205262\n",
      "11205285\n",
      "11205312\n",
      "11205314\n",
      "11205354\n",
      "11205357\n",
      "11205364\n",
      "11205372\n",
      "11205374\n",
      "11205378\n",
      "11205385\n",
      "11205391\n",
      "11205456\n",
      "11205468\n",
      "11205472\n",
      "11205539\n",
      "11205628\n",
      "11205631\n",
      "11205635\n",
      "11205719\n",
      "11205745\n",
      "11205754\n",
      "11205772\n",
      "11205779\n",
      "11205817\n",
      "11205851\n",
      "11205884\n",
      "11205934\n",
      "11205970\n",
      "11205972\n",
      "11206001\n",
      "11206014\n",
      "11206018\n",
      "11206019\n",
      "11206040\n",
      "11206042\n",
      "11206082\n",
      "11206132\n",
      "11206155\n",
      "11206182\n",
      "11206184\n",
      "11206224\n",
      "11206227\n",
      "11206234\n",
      "11206242\n",
      "11206244\n",
      "11206248\n",
      "11206255\n",
      "11206261\n",
      "11206326\n",
      "11206338\n",
      "11206342\n",
      "11206409\n",
      "11206498\n",
      "11206501\n",
      "11206505\n",
      "11206589\n",
      "11206615\n",
      "11206624\n",
      "11206642\n",
      "11206649\n",
      "11206687\n",
      "11206721\n",
      "11206754\n",
      "11206804\n",
      "11206840\n",
      "11206842\n",
      "11206871\n",
      "11206884\n",
      "11206888\n",
      "11206889\n",
      "11206910\n",
      "11206912\n",
      "11206952\n",
      "11207002\n",
      "11207025\n",
      "11207052\n",
      "11207054\n",
      "11207094\n",
      "11207097\n",
      "11207104\n",
      "11207112\n",
      "11207114\n",
      "11207118\n",
      "11207125\n",
      "11207131\n",
      "11207196\n",
      "11207208\n",
      "11207212\n",
      "11207279\n",
      "11207368\n",
      "11207371\n",
      "11207375\n",
      "11207459\n",
      "11207485\n",
      "11207494\n",
      "11207512\n",
      "11207519\n",
      "11207557\n",
      "11207591\n",
      "11207624\n",
      "11207674\n",
      "11207710\n",
      "11207712\n",
      "11207741\n",
      "11207754\n",
      "11207758\n",
      "11207759\n",
      "11207780\n",
      "11207782\n",
      "11207822\n",
      "11207872\n",
      "11207895\n",
      "11207922\n",
      "11207924\n",
      "11207964\n",
      "11207967\n",
      "11207974\n",
      "11207982\n",
      "11207984\n",
      "11207988\n",
      "11207995\n",
      "11208001\n",
      "11208066\n",
      "11208078\n",
      "11208082\n",
      "11208149\n",
      "11208238\n",
      "11208241\n",
      "11208245\n",
      "11208329\n",
      "11208355\n",
      "11208364\n",
      "11208382\n",
      "11208389\n",
      "11208427\n",
      "11208461\n",
      "11208494\n",
      "11208544\n",
      "11208580\n",
      "11208582\n",
      "11208611\n",
      "11208624\n",
      "11208628\n",
      "11208629\n",
      "11208650\n",
      "11208652\n",
      "11208692\n",
      "11208742\n",
      "11208765\n",
      "11208792\n",
      "11208794\n",
      "11208834\n",
      "11208837\n",
      "11208844\n",
      "11208852\n",
      "11208854\n",
      "11208858\n",
      "11208865\n",
      "11208871\n",
      "11208936\n",
      "11208948\n",
      "11208952\n",
      "11209019\n",
      "11209108\n",
      "11209111\n",
      "11209115\n",
      "11209199\n",
      "11209225\n",
      "11209234\n",
      "11209252\n",
      "11209259\n",
      "11209297\n",
      "11209331\n",
      "11209364\n",
      "11209414\n",
      "11209450\n",
      "11209452\n",
      "11209481\n",
      "11209494\n",
      "11209498\n",
      "11209499\n",
      "11209520\n",
      "11209522\n",
      "11209562\n",
      "11209612\n",
      "11209635\n",
      "11209662\n",
      "11209664\n",
      "11209704\n",
      "11209707\n",
      "11209714\n",
      "11209722\n",
      "11209724\n",
      "11209728\n",
      "11209735\n",
      "11209741\n",
      "11209806\n",
      "11209818\n",
      "11209822\n",
      "11209889\n",
      "11209978\n",
      "11209981\n",
      "11209985\n",
      "11210069\n",
      "11210095\n",
      "11210104\n",
      "11210122\n",
      "11210129\n",
      "11210167\n",
      "11210201\n",
      "11210234\n",
      "11210284\n",
      "11210320\n",
      "11210322\n",
      "11210351\n",
      "11210364\n",
      "11210368\n",
      "11210369\n",
      "11210390\n",
      "11210392\n",
      "11210432\n",
      "11210482\n",
      "11210505\n",
      "11210532\n",
      "11210534\n",
      "11210574\n",
      "11210577\n",
      "11210584\n",
      "11210592\n",
      "11210594\n",
      "11210598\n",
      "11210605\n",
      "11210611\n",
      "11210676\n",
      "11210688\n",
      "11210692\n",
      "11210759\n",
      "11210848\n",
      "11210851\n",
      "11210855\n",
      "11210939\n",
      "11210965\n",
      "11210974\n",
      "11210992\n",
      "11210999\n",
      "11211037\n",
      "11211071\n",
      "11211104\n",
      "11211154\n",
      "11211190\n",
      "11211192\n",
      "11211221\n",
      "11211234\n",
      "11211238\n",
      "11211239\n",
      "11211260\n",
      "11211262\n",
      "11211302\n",
      "11211352\n",
      "11211375\n",
      "11211402\n",
      "11211404\n",
      "11211444\n",
      "11211447\n",
      "11211454\n",
      "11211462\n",
      "11211464\n",
      "11211468\n",
      "11211475\n",
      "11211481\n",
      "11211546\n",
      "11211558\n",
      "11211562\n",
      "11211629\n",
      "11211718\n",
      "11211721\n",
      "11211725\n",
      "11211809\n",
      "11211835\n",
      "11211844\n",
      "11211862\n",
      "11211869\n",
      "11211907\n",
      "11211941\n",
      "11211974\n",
      "11212024\n",
      "11212060\n",
      "11212062\n",
      "11212091\n",
      "11212104\n",
      "11212108\n",
      "11212109\n",
      "11212130\n",
      "11212132\n",
      "11212172\n",
      "11212222\n",
      "11212245\n",
      "11212272\n",
      "11212274\n",
      "11212314\n",
      "11212317\n",
      "11212324\n",
      "11212332\n",
      "11212334\n",
      "11212338\n",
      "11212345\n",
      "11212351\n",
      "11212416\n",
      "11212428\n",
      "11212432\n",
      "11212499\n",
      "11212588\n",
      "11212591\n",
      "11212595\n",
      "11212679\n",
      "11212705\n",
      "11212714\n",
      "11212732\n",
      "11212739\n",
      "11212777\n",
      "11212811\n",
      "11212844\n",
      "11212894\n",
      "11212930\n",
      "11212932\n",
      "11212961\n",
      "11212974\n",
      "11212978\n",
      "11212979\n",
      "11213000\n",
      "11213002\n",
      "11213042\n",
      "11213092\n",
      "11213115\n",
      "11213142\n",
      "11213144\n",
      "11213184\n",
      "11213187\n",
      "11213194\n",
      "11213202\n",
      "11213204\n",
      "11213208\n",
      "11213215\n",
      "11213221\n",
      "11213286\n",
      "11213298\n",
      "11213302\n",
      "11213369\n",
      "11213458\n",
      "11213461\n",
      "11213465\n",
      "11213549\n",
      "11213575\n",
      "11213584\n",
      "11213602\n",
      "11213609\n",
      "11213647\n",
      "11213681\n",
      "11213714\n",
      "11213764\n",
      "11213800\n",
      "11213802\n",
      "11213831\n",
      "11213844\n",
      "11213848\n",
      "11213849\n",
      "11213870\n",
      "11213872\n",
      "11213912\n",
      "11213962\n",
      "11213985\n",
      "11214012\n",
      "11214014\n",
      "11214054\n",
      "11214057\n",
      "11214064\n",
      "11214072\n",
      "11214074\n",
      "11214078\n",
      "11214085\n",
      "11214091\n",
      "11214156\n",
      "11214168\n",
      "11214172\n",
      "11214239\n",
      "11214328\n",
      "11214331\n",
      "11214335\n",
      "11214419\n",
      "11214445\n",
      "11214454\n",
      "11214472\n",
      "11214479\n",
      "11214517\n",
      "11214551\n",
      "11214584\n",
      "11214634\n",
      "11214670\n",
      "11214672\n",
      "11214701\n",
      "11214714\n",
      "11214718\n",
      "11214719\n",
      "11214740\n",
      "11214742\n",
      "11214782\n",
      "11214832\n",
      "11214855\n",
      "11214882\n",
      "11214884\n",
      "11214924\n",
      "11214927\n",
      "11214934\n",
      "11214942\n",
      "11214944\n",
      "11214948\n",
      "11214955\n",
      "11214961\n",
      "11215026\n",
      "11215038\n",
      "11215042\n",
      "11215109\n",
      "11215198\n",
      "11215201\n",
      "11215205\n",
      "11215289\n",
      "11215315\n",
      "11215324\n",
      "11215342\n",
      "11215349\n",
      "11215387\n",
      "11215421\n",
      "11215454\n",
      "11215504\n",
      "11215540\n",
      "11215542\n",
      "11215571\n",
      "11215584\n",
      "11215588\n",
      "11215589\n",
      "11215610\n",
      "11215612\n",
      "11215652\n",
      "11215702\n",
      "11215725\n",
      "11215752\n",
      "11215754\n",
      "11215794\n",
      "11215797\n",
      "11215804\n",
      "11215812\n",
      "11215814\n",
      "11215818\n",
      "11215825\n",
      "11215831\n",
      "11215896\n",
      "11215908\n",
      "11215912\n",
      "11215979\n",
      "11216068\n",
      "11216071\n",
      "11216075\n",
      "11216159\n",
      "11216185\n",
      "11216194\n",
      "11216212\n",
      "11216219\n",
      "11216257\n",
      "11216291\n",
      "11216324\n",
      "11216374\n",
      "11216410\n",
      "11216412\n",
      "11216441\n",
      "11216454\n",
      "11216458\n",
      "11216459\n",
      "11216480\n",
      "11216482\n",
      "11216522\n",
      "11216572\n",
      "11216595\n",
      "11216622\n",
      "11216624\n",
      "11216664\n",
      "11216667\n",
      "11216674\n",
      "11216682\n",
      "11216684\n",
      "11216688\n",
      "11216695\n",
      "11216701\n",
      "11216766\n",
      "11216778\n",
      "11216782\n",
      "11216849\n",
      "11216938\n",
      "11216941\n",
      "11216945\n",
      "11217029\n",
      "11217055\n",
      "11217064\n",
      "11217082\n",
      "11217089\n",
      "11217127\n",
      "11217161\n",
      "11217194\n",
      "11217244\n",
      "11217280\n",
      "11217282\n",
      "11217311\n",
      "11217324\n",
      "11217328\n",
      "11217329\n",
      "11217350\n",
      "11217352\n",
      "11217392\n",
      "11217442\n",
      "11217465\n",
      "11217492\n",
      "11217494\n",
      "11217534\n",
      "11217537\n",
      "11217544\n",
      "11217552\n",
      "11217554\n",
      "11217558\n",
      "11217565\n",
      "11217571\n",
      "11217636\n",
      "11217648\n",
      "11217652\n",
      "11217719\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-736b6957b7b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Delete the elements in which data are missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "X,Y = from_list_to_dataset(dataset)\n",
    "n = len(X[0])\n",
    "# Delete the elements in which data are missing\n",
    "for i in range(len(X)) :\n",
    "  element = X[i]\n",
    "  if len(element)!= n:\n",
    "    print(i)\n",
    "    del X[i]\n",
    "    del Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 17385,
     "status": "ok",
     "timestamp": 1628608507889,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "uYbV6Kckz_PE"
   },
   "outputs": [],
   "source": [
    "# Convert everything into a numpy array\n",
    "X = np.asarray(X).astype(\"float32\")\n",
    "Y = np.asarray(Y).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 8821,
     "status": "ok",
     "timestamp": 1628608516695,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "bm8Ej7c70BJs"
   },
   "outputs": [],
   "source": [
    "# Split the dataset in 2 : 1 for the training, one for the test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42,stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1628608517013,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "t5F7QHWs0FN6",
    "outputId": "a1736cf9-3ad7-48e6-cc8a-9e140d65de65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"multi_layer_regularization\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden (Dense)               (None, 10)                90        \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 112\n",
      "Trainable params: 112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the neural network\n",
    "# Naming it \n",
    "multi_layer_reg_model = tf.keras.Sequential(name='multi_layer_regularization')\n",
    "# Adding the input layer : as much neuron as input\n",
    "multi_layer_reg_model.add(tf.keras.layers.Input(X.shape[1]))\n",
    "# Adding the hidden layer \n",
    "multi_layer_reg_model.add(tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l1(1), name='hidden'))\n",
    "# Adding the ouput layer : as much neurons as potential output\n",
    "multi_layer_reg_model.add(tf.keras.layers.Dense(2, activation='softmax', name='output'))\n",
    "# Defining the metric to use : \n",
    "# we choose sensititivity since the we want our neural network to classify the one who already invest as investors\n",
    "# But the one who didn't invest yet may still be potential investor \n",
    "multi_layer_reg_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[sensitivity])\n",
    "# Recap of the the model parameters\n",
    "multi_layer_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1628608522467,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "oAFXJjzI0Is9"
   },
   "outputs": [],
   "source": [
    "# The training phase will stop if the sensitivity don't improve of more than 1e-2 in 2 epochs\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1e-2)\n",
    "# Save only the best model\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('multi_layer_best.h5', monitor='sensitivity', verbose=1 ,mode ='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "xsPRcQ2vVnUl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 22s 3ms/step - loss: 1.1223 - sensitivity: 0.9629 - val_loss: 0.0292 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0238 - sensitivity: 0.9608 - val_loss: 0.0237 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9596 - val_loss: 0.0214 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9627 - val_loss: 0.0223 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9670 - val_loss: 0.0216 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9565 - val_loss: 0.0214 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0210 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9698 - val_loss: 0.0215 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9641 - val_loss: 0.0228 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9620 - val_loss: 0.0212 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 93s 1ms/step - loss: 0.0212 - sensitivity: 0.0516\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9622 - val_loss: 0.0221 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9603 - val_loss: 0.0229 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9620 - val_loss: 0.0209 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9613 - val_loss: 0.0216 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9655 - val_loss: 0.0209 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9608 - val_loss: 0.0228 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9572 - val_loss: 0.0222 - val_sensitivity: 0.9501\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 00007: early stopping\n",
      "87639/87639 [==============================] - 93s 1ms/step - loss: 0.0222 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9620 - val_loss: 0.0219 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9624 - val_loss: 0.0218 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9677 - val_loss: 0.0213 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9591 - val_loss: 0.0217 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9636 - val_loss: 0.0213 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9584 - val_loss: 0.0222 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9634 - val_loss: 0.0225 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 00007: early stopping\n",
      "87639/87639 [==============================] - 95s 1ms/step - loss: 0.0225 - sensitivity: 0.0515\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9641 - val_loss: 0.0219 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9624 - val_loss: 0.0210 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9598 - val_loss: 0.0223 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9584 - val_loss: 0.0228 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9653 - val_loss: 0.0217 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9624 - val_loss: 0.0211 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9613 - val_loss: 0.0218 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9627 - val_loss: 0.0218 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9658 - val_loss: 0.0216 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9662 - val_loss: 0.0218 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 94s 1ms/step - loss: 0.0218 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9653 - val_loss: 0.0224 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9639 - val_loss: 0.0221 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9629 - val_loss: 0.0213 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9696 - val_loss: 0.0214 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9634 - val_loss: 0.0219 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9620 - val_loss: 0.0218 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9641 - val_loss: 0.0217 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9639 - val_loss: 0.0214 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9601 - val_loss: 0.0216 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9665 - val_loss: 0.0221 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 96s 1ms/step - loss: 0.0221 - sensitivity: 0.0516\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9605 - val_loss: 0.0215 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9598 - val_loss: 0.0218 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9660 - val_loss: 0.0221 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9641 - val_loss: 0.0222 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9608 - val_loss: 0.0211 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9632 - val_loss: 0.0204 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9579 - val_loss: 0.0216 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9579 - val_loss: 0.0229 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9613 - val_loss: 0.0218 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9565 - val_loss: 0.0215 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 98s 1ms/step - loss: 0.0215 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9698 - val_loss: 0.0227 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9617 - val_loss: 0.0223 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9613 - val_loss: 0.0210 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9636 - val_loss: 0.0217 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9622 - val_loss: 0.0226 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9577 - val_loss: 0.0215 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9639 - val_loss: 0.0223 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9615 - val_loss: 0.0209 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9610 - val_loss: 0.0217 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9648 - val_loss: 0.0219 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 100s 1ms/step - loss: 0.0220 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9634 - val_loss: 0.0217 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9632 - val_loss: 0.0215 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9601 - val_loss: 0.0213 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9596 - val_loss: 0.0226 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9629 - val_loss: 0.0221 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9655 - val_loss: 0.0214 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9667 - val_loss: 0.0222 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9655 - val_loss: 0.0216 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9636 - val_loss: 0.0218 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9584 - val_loss: 0.0213 - val_sensitivity: 0.9622\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 99s 1ms/step - loss: 0.0213 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9653 - val_loss: 0.0210 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9636 - val_loss: 0.0225 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9639 - val_loss: 0.0213 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9598 - val_loss: 0.0219 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9643 - val_loss: 0.0215 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9653 - val_loss: 0.0215 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9629 - val_loss: 0.0216 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9686 - val_loss: 0.0214 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0220 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9665 - val_loss: 0.0213 - val_sensitivity: 0.9615\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 103s 1ms/step - loss: 0.0213 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0216 - sensitivity: 0.9598 - val_loss: 0.0212 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0212 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9665 - val_loss: 0.0212 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9605 - val_loss: 0.0222 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9589 - val_loss: 0.0227 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9651 - val_loss: 0.0211 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0215 - sensitivity: 0.9651 - val_loss: 0.0216 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0215 - sensitivity: 0.9615 - val_loss: 0.0219 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0215 - sensitivity: 0.9639 - val_loss: 0.0211 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0215 - sensitivity: 0.9627 - val_loss: 0.0206 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 102s 1ms/step - loss: 0.0205 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0215 - sensitivity: 0.9627 - val_loss: 0.0220 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0215 - sensitivity: 0.9655 - val_loss: 0.0216 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9620 - val_loss: 0.0217 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9596 - val_loss: 0.0218 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0224 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9610 - val_loss: 0.0218 - val_sensitivity: 0.9665\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 00006: early stopping\n",
      "87639/87639 [==============================] - 102s 1ms/step - loss: 0.0219 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9655 - val_loss: 0.0216 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9653 - val_loss: 0.0212 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9677 - val_loss: 0.0213 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9660 - val_loss: 0.0219 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9643 - val_loss: 0.0204 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9727 - val_loss: 0.0226 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9662 - val_loss: 0.0222 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 00007: early stopping\n",
      "87639/87639 [==============================] - 100s 1ms/step - loss: 0.0222 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0219 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9624 - val_loss: 0.0220 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9563 - val_loss: 0.0203 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9608 - val_loss: 0.0211 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9641 - val_loss: 0.0220 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9643 - val_loss: 0.0221 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9677 - val_loss: 0.0210 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9622 - val_loss: 0.0215 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9634 - val_loss: 0.0223 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9641 - val_loss: 0.0208 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "Epoch 00010: early stopping\n",
      "87639/87639 [==============================] - 104s 1ms/step - loss: 0.0208 - sensitivity: 0.0515\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0216 - sensitivity: 0.9677 - val_loss: 0.0218 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0216 - sensitivity: 0.9582 - val_loss: 0.0228 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0216 - sensitivity: 0.9594 - val_loss: 0.0216 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0215 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0216 - sensitivity: 0.9624 - val_loss: 0.0204 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0216 - sensitivity: 0.9634 - val_loss: 0.0215 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0222 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 19s 4ms/step - loss: 0.0216 - sensitivity: 0.9608 - val_loss: 0.0216 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 22s 5ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0216 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0216 - sensitivity: 0.9603 - val_loss: 0.0217 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 138s 2ms/step - loss: 0.0217 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0216 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0216 - sensitivity: 0.9667 - val_loss: 0.0216 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9610 - val_loss: 0.0207 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0215 - sensitivity: 0.9655 - val_loss: 0.0220 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0215 - sensitivity: 0.9634 - val_loss: 0.0217 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9634 - val_loss: 0.0215 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9639 - val_loss: 0.0204 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0215 - sensitivity: 0.9601 - val_loss: 0.0223 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0215 - sensitivity: 0.9603 - val_loss: 0.0221 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9620 - val_loss: 0.0220 - val_sensitivity: 0.9644\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 131s 1ms/step - loss: 0.0220 - sensitivity: 0.0515\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9605 - val_loss: 0.0214 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0215 - sensitivity: 0.9620 - val_loss: 0.0212 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 19s 5ms/step - loss: 0.0215 - sensitivity: 0.9624 - val_loss: 0.0202 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9627 - val_loss: 0.0210 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9594 - val_loss: 0.0215 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 19s 4ms/step - loss: 0.0215 - sensitivity: 0.9627 - val_loss: 0.0226 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 20s 5ms/step - loss: 0.0215 - sensitivity: 0.9617 - val_loss: 0.0222 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0215 - sensitivity: 0.9643 - val_loss: 0.0215 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9658 - val_loss: 0.0213 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9610 - val_loss: 0.0211 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 141s 2ms/step - loss: 0.0211 - sensitivity: 0.0515\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9615 - val_loss: 0.0216 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9617 - val_loss: 0.0223 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0215 - sensitivity: 0.9667 - val_loss: 0.0217 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9572 - val_loss: 0.0220 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9648 - val_loss: 0.0207 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9662 - val_loss: 0.0220 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 19s 4ms/step - loss: 0.0215 - sensitivity: 0.9627 - val_loss: 0.0214 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9696 - val_loss: 0.0217 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9655 - val_loss: 0.0221 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9665 - val_loss: 0.0209 - val_sensitivity: 0.9572\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 142s 2ms/step - loss: 0.0209 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0215 - sensitivity: 0.9632 - val_loss: 0.0220 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9629 - val_loss: 0.0212 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0215 - sensitivity: 0.9610 - val_loss: 0.0211 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 19s 5ms/step - loss: 0.0215 - sensitivity: 0.9653 - val_loss: 0.0213 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9620 - val_loss: 0.0217 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0215 - sensitivity: 0.9610 - val_loss: 0.0219 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 18s 4ms/step - loss: 0.0215 - sensitivity: 0.9660 - val_loss: 0.0217 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0215 - sensitivity: 0.9589 - val_loss: 0.0213 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 19s 4ms/step - loss: 0.0216 - sensitivity: 0.9674 - val_loss: 0.0219 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 17s 4ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0226 - val_sensitivity: 0.9544\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 152s 2ms/step - loss: 0.0227 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9689 - val_loss: 0.0212 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9639 - val_loss: 0.0218 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9639 - val_loss: 0.0207 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9610 - val_loss: 0.0220 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9641 - val_loss: 0.0224 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9596 - val_loss: 0.0221 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9620 - val_loss: 0.0212 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9643 - val_loss: 0.0219 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9636 - val_loss: 0.0217 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9639 - val_loss: 0.0208 - val_sensitivity: 0.9601\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 105s 1ms/step - loss: 0.0208 - sensitivity: 0.0512\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9627 - val_loss: 0.0224 - val_sensitivity: 0.9594\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9653 - val_loss: 0.0223 - val_sensitivity: 0.9594\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9632 - val_loss: 0.0220 - val_sensitivity: 0.9594\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9653 - val_loss: 0.0218 - val_sensitivity: 0.9594\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9629 - val_loss: 0.0210 - val_sensitivity: 0.9594\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9641 - val_loss: 0.0214 - val_sensitivity: 0.9594\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 00006: early stopping\n",
      "87639/87639 [==============================] - 104s 1ms/step - loss: 0.0214 - sensitivity: 0.0516\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9622 - val_loss: 0.0216 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9658 - val_loss: 0.0221 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9591 - val_loss: 0.0216 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9681 - val_loss: 0.0210 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9608 - val_loss: 0.0212 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9601 - val_loss: 0.0222 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 00006: early stopping\n",
      "87639/87639 [==============================] - 103s 1ms/step - loss: 0.0222 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9658 - val_loss: 0.0211 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9591 - val_loss: 0.0234 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9608 - val_loss: 0.0218 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9570 - val_loss: 0.0219 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9648 - val_loss: 0.0213 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9665 - val_loss: 0.0212 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9639 - val_loss: 0.0218 - val_sensitivity: 0.9715\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 00007: early stopping\n",
      "87639/87639 [==============================] - 106s 1ms/step - loss: 0.0218 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9646 - val_loss: 0.0220 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9622 - val_loss: 0.0224 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9622 - val_loss: 0.0221 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9603 - val_loss: 0.0216 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9603 - val_loss: 0.0215 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9646 - val_loss: 0.0219 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9643 - val_loss: 0.0221 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9627 - val_loss: 0.0216 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9615 - val_loss: 0.0216 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9653 - val_loss: 0.0207 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 105s 1ms/step - loss: 0.0207 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9610 - val_loss: 0.0222 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9670 - val_loss: 0.0221 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9636 - val_loss: 0.0229 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9613 - val_loss: 0.0229 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 16s 4ms/step - loss: 0.0217 - sensitivity: 0.9632 - val_loss: 0.0225 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0217 - sensitivity: 0.9632 - val_loss: 0.0207 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9639 - val_loss: 0.0214 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9658 - val_loss: 0.0218 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9651 - val_loss: 0.0221 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9641 - val_loss: 0.0220 - val_sensitivity: 0.9587\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 112s 1ms/step - loss: 0.0220 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9589 - val_loss: 0.0209 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9634 - val_loss: 0.0220 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9696 - val_loss: 0.0213 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9622 - val_loss: 0.0215 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9591 - val_loss: 0.0226 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 15s 4ms/step - loss: 0.0217 - sensitivity: 0.9658 - val_loss: 0.0223 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9615 - val_loss: 0.0218 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9679 - val_loss: 0.0220 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0217 - sensitivity: 0.9567 - val_loss: 0.0227 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0217 - sensitivity: 0.9670 - val_loss: 0.0224 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 106s 1ms/step - loss: 0.0224 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0217 - sensitivity: 0.9646 - val_loss: 0.0219 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0217 - sensitivity: 0.9617 - val_loss: 0.0219 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9651 - val_loss: 0.0213 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9603 - val_loss: 0.0210 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9579 - val_loss: 0.0212 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0216 - sensitivity: 0.9624 - val_loss: 0.0217 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9620 - val_loss: 0.0230 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9672 - val_loss: 0.0218 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9636 - val_loss: 0.0215 - val_sensitivity: 0.9629\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 00009: early stopping\n",
      "87639/87639 [==============================] - 101s 1ms/step - loss: 0.0215 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9622 - val_loss: 0.0215 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9596 - val_loss: 0.0211 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0217 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0223 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9641 - val_loss: 0.0219 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9674 - val_loss: 0.0221 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0208 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9636 - val_loss: 0.0212 - val_sensitivity: 0.9722\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 00008: early stopping\n",
      "87639/87639 [==============================] - 102s 1ms/step - loss: 0.0213 - sensitivity: 0.0516\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9705 - val_loss: 0.0221 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9655 - val_loss: 0.0218 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9660 - val_loss: 0.0221 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0212 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9636 - val_loss: 0.0208 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9627 - val_loss: 0.0216 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9629 - val_loss: 0.0218 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9613 - val_loss: 0.0232 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9601 - val_loss: 0.0213 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9655 - val_loss: 0.0220 - val_sensitivity: 0.9636\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 103s 1ms/step - loss: 0.0220 - sensitivity: 0.0516\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9622 - val_loss: 0.0205 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9624 - val_loss: 0.0207 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0228 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0222 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9651 - val_loss: 0.0220 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0216 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9662 - val_loss: 0.0204 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9679 - val_loss: 0.0219 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9591 - val_loss: 0.0213 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9620 - val_loss: 0.0214 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "Epoch 00010: early stopping\n",
      "87639/87639 [==============================] - 108s 1ms/step - loss: 0.0214 - sensitivity: 0.0515\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9634 - val_loss: 0.0226 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9613 - val_loss: 0.0218 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9677 - val_loss: 0.0216 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 14s 3ms/step - loss: 0.0216 - sensitivity: 0.9601 - val_loss: 0.0212 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9708 - val_loss: 0.0219 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9627 - val_loss: 0.0220 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9586 - val_loss: 0.0220 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0226 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9639 - val_loss: 0.0206 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9624 - val_loss: 0.0212 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 108s 1ms/step - loss: 0.0212 - sensitivity: 0.0516\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9651 - val_loss: 0.0224 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9622 - val_loss: 0.0220 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0219 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9662 - val_loss: 0.0220 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9653 - val_loss: 0.0213 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0223 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9660 - val_loss: 0.0211 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9639 - val_loss: 0.0223 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9615 - val_loss: 0.0213 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 00009: early stopping\n",
      "87639/87639 [==============================] - 102s 1ms/step - loss: 0.0212 - sensitivity: 0.0514\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9608 - val_loss: 0.0219 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9693 - val_loss: 0.0217 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9655 - val_loss: 0.0209 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9636 - val_loss: 0.0226 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9634 - val_loss: 0.0219 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9610 - val_loss: 0.0222 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9653 - val_loss: 0.0215 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9603 - val_loss: 0.0207 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9632 - val_loss: 0.0209 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9684 - val_loss: 0.0219 - val_sensitivity: 0.9515\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 104s 1ms/step - loss: 0.0219 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9643 - val_loss: 0.0230 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9643 - val_loss: 0.0219 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9603 - val_loss: 0.0220 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9646 - val_loss: 0.0216 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9617 - val_loss: 0.0207 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9636 - val_loss: 0.0217 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9610 - val_loss: 0.0225 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9658 - val_loss: 0.0218 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9622 - val_loss: 0.0219 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9613 - val_loss: 0.0214 - val_sensitivity: 0.9608\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "87639/87639 [==============================] - 104s 1ms/step - loss: 0.0214 - sensitivity: 0.0513\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9610 - val_loss: 0.0205 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9624 - val_loss: 0.0215 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9598 - val_loss: 0.0227 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 11s 3ms/step - loss: 0.0216 - sensitivity: 0.9575 - val_loss: 0.0221 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9681 - val_loss: 0.0214 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9620 - val_loss: 0.0218 - val_sensitivity: 0.9651\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 00006: early stopping\n",
      "87639/87639 [==============================] - 107s 1ms/step - loss: 0.0218 - sensitivity: 0.0515\n",
      "Epoch 1/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9634 - val_loss: 0.0215 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00001: saving model to multi_layer_best.h5\n",
      "Epoch 2/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9598 - val_loss: 0.0214 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00002: saving model to multi_layer_best.h5\n",
      "Epoch 3/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9665 - val_loss: 0.0222 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00003: saving model to multi_layer_best.h5\n",
      "Epoch 4/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9672 - val_loss: 0.0220 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00004: saving model to multi_layer_best.h5\n",
      "Epoch 5/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9596 - val_loss: 0.0231 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00005: saving model to multi_layer_best.h5\n",
      "Epoch 6/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9603 - val_loss: 0.0211 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00006: saving model to multi_layer_best.h5\n",
      "Epoch 7/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0208 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00007: saving model to multi_layer_best.h5\n",
      "Epoch 8/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9615 - val_loss: 0.0221 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00008: saving model to multi_layer_best.h5\n",
      "Epoch 9/10\n",
      "4207/4207 [==============================] - 13s 3ms/step - loss: 0.0216 - sensitivity: 0.9651 - val_loss: 0.0219 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00009: saving model to multi_layer_best.h5\n",
      "Epoch 10/10\n",
      "4207/4207 [==============================] - 12s 3ms/step - loss: 0.0216 - sensitivity: 0.9648 - val_loss: 0.0215 - val_sensitivity: 0.9537\n",
      "\n",
      "Epoch 00010: saving model to multi_layer_best.h5\n",
      "77883/87639 [=========================>....] - ETA: 11s - loss: 0.0215 - sensitivity: 0.0518"
     ]
    }
   ],
   "source": [
    "# Split the dataset in 2 : 1 for the training, one for the test\n",
    "list_sensitivity=[]\n",
    "for i in range(100):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state= i ,stratify = Y)\n",
    "# training the model \n",
    "  multi_layer_reg_train = multi_layer_reg_model.fit(X_train, y_train ,callbacks=[earlystop,checkpoint], epochs=10, batch_size=2000, validation_data=(X_test,y_test))\n",
    "  loss, acc = multi_layer_reg_model.evaluate(X_test, y_test)\n",
    "  list_sensitivity.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "aborted",
     "timestamp": 1628602413460,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "pV7XgQ_s0LGo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 493,
     "status": "aborted",
     "timestamp": 1628602413460,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "dO_hKF2o0Nk6"
   },
   "outputs": [],
   "source": [
    "fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(20,7))\n",
    "\n",
    "loss_ax.set_title('Loss')\n",
    "loss_ax.plot(multi_layer_reg_train.history['loss'], '-r', label='Train')\n",
    "\n",
    "acc_ax.set_title('Accuracy')\n",
    "acc_ax.plot(multi_layer_reg_train.history['sensitivity'], '-r', label='Train')\n",
    "\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "aborted",
     "timestamp": 1628602413461,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "e057ROoz-rZD"
   },
   "outputs": [],
   "source": [
    "def predict_investor(company,company_dictionary,dictionnary_fund_deal_name,strategy_dictionary,dictionary_company_portfolio, model):\n",
    "  list_potential_investor = []\n",
    "  company_attributes = company_dictionary[company]\n",
    "  for fund in dictionary_fund_deal_name.keys():\n",
    "    vector_for_predictions = []\n",
    "    \n",
    "    # If we have a strategy related to this fund\n",
    "    if fund in strategy_dictionary.keys():\n",
    "      \n",
    "      # if the fund didn't already invest in the company\n",
    "      #if fund not in dictionary_company_portfolio[company]:\n",
    "      list_strategy = strategy_dictionary[fund]\n",
    "      line = [company, fund]\n",
    "      number_strategy = len(list_strategy)\n",
    "      number_matching_strategy = 0\n",
    "      histo = history(company, fund, dictionary_fund_deal_name, company_dictionary)\n",
    "      for element in histo:\n",
    "          line.append(element)\n",
    "\n",
    "        # For each strategy of the fund\n",
    "      for strategy in list_strategy:\n",
    "        matching = True\n",
    "        \n",
    "      # If the the sector from the company corresponds to one in the strategy\n",
    "        if (company_dictionary[company][1] not in strategy[0]) and (1 not in strategy[0]):\n",
    "          matching = False\n",
    "\n",
    "        # If the localisation\n",
    "        if company_dictionary[company][0] not in strategy[1]:\n",
    "          matching = False\n",
    "\n",
    "        # If the region\n",
    "        if company_dictionary[company][3] not in strategy[2]:\n",
    "          matching = False\n",
    "        \n",
    "        # We consider that the company fit to the strategy \n",
    "        if matching:\n",
    "          number_matching_strategy += 1\n",
    "        \n",
    "        # We want to know the proportion of strategy into which the company fit in \n",
    "      score_matching = number_matching_strategy / number_strategy\n",
    "      line.append(score_matching)\n",
    "        \n",
    "        # We delete the name of the fund the company\n",
    "      del line[0]\n",
    "      del line[0]\n",
    "        \n",
    "        # We predict if the fund may be a potential investor with our model\n",
    "      vector_for_predictions = np.array(line).reshape(1,11)\n",
    "      predictions = model.predict(vector_for_predictions)\n",
    "      value_predictions = np.argmax(predictions, axis=1)[0]\n",
    "      print(value_predictions)\n",
    "        \n",
    "        # If it is we add it to the list\n",
    "      if value_predictions ==1 :\n",
    "        list_potential_investor.append(fund)\n",
    "  return list_potential_investor \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 495,
     "status": "aborted",
     "timestamp": 1628602413462,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "ztPP4h9BL0AY"
   },
   "outputs": [],
   "source": [
    "dictionnary_company = company_dictionary\n",
    "company = 'Multiplast-Dulary'\n",
    "model = multi_layer_reg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "aborted",
     "timestamp": 1628602413462,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "xRmYdhZrUWhA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "aborted",
     "timestamp": 1628602413463,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "dS_EriMDjcZz"
   },
   "outputs": [],
   "source": [
    "loss, acc = multi_layer_reg_model.evaluate(X_test, y_test)\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 495,
     "status": "aborted",
     "timestamp": 1628602413464,
     "user": {
      "displayName": "Jean Sébastien Abessouguie Bayiha",
      "photoUrl": "",
      "userId": "10491268087025045829"
     },
     "user_tz": -120
    },
    "id": "Jo5P0c4oNIqh"
   },
   "outputs": [],
   "source": [
    "predict_investor(company,company_dictionary,dictionary_fund_deal_name,strategy_dictionary,dictionary_company_portfolio, model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZQrzjDzkM2vIBGVpSNGnx",
   "name": "main.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
